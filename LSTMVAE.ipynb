{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data, datasets\n",
    "from torchtext.vocab import FastText\n",
    "\n",
    "import os\n",
    "import math\n",
    "import MeCab\n",
    "import shutil\n",
    "\n",
    "device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 1\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "mecab = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "# BLEU\n",
    "\n",
    "from Util.selfbleu import CalcSelfBLEU\n",
    "from Util.bleu import calc_all_bleu\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchwordemb\n",
    "w2v, vec = torchwordemb.load_word2vec_text('./data/vector/model.vec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPER PARAMETER\n",
    "BATCH_SIZE=100\n",
    "EMBED_SIZE=300\n",
    "LSTM_HIDDEN_SIZE=300\n",
    "AE_EPOCHS=100\n",
    "REPORT_INTERVAL=1\n",
    "ae_lr=1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL NAME\n",
    "data_source=\"wiki\"\n",
    "feature=\"only_ja_small\"\n",
    "# feature=\"only_ja_sample\"\n",
    "# data_source=\"orphans\"\n",
    "# feature=\"all\"\n",
    "\n",
    "log_dir='./log/{}/'.format(\"_\".join([\"train\",data_source,feature]))\n",
    "if os.path.isdir(log_dir):\n",
    "    shutil.rmtree(log_dir)\n",
    "os.mkdir(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_source==\"wiki\" or data_source==\"orphans\":\n",
    "    MAX_LEN=20 #paddingをバッチにかかわらず固定長にする\n",
    "    TEXT = data.Field(sequential=True, lower=True, include_lengths=True, init_token=\"<SOS>\",eos_token=\"<EOS>\", fix_length=MAX_LEN, batch_first=True)\n",
    "    train, val, test = data.TabularDataset.splits(\n",
    "        path='./data/foruse', train=\"_\".join([\"train\",data_source,feature])+'.tsv',\n",
    "        validation=\"_\".join([\"eval\",data_source,feature])+'.tsv',test=\"_\".join([\"test\",data_source,feature])+'.tsv', \n",
    "        format='tsv',\n",
    "        fields=[('Text', TEXT),])\n",
    "#     TEXT.build_vocab(train, vectors=FastText(language=\"ja\"))\n",
    "    TEXT.build_vocab(train)\n",
    "    TEXT.vocab.set_vectors(stoi=w2v, vectors=vec, dim=300)\n",
    "    train_iter, val_iter, test_iter = data.Iterator.splits((train, val, test), batch_sizes=(BATCH_SIZE, BATCH_SIZE, BATCH_SIZE),sort = False, device=torch.device(device))\n",
    "    vocab_size=TEXT.vocab.vectors.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [-0.2223, -0.1124, -0.1073,  ..., -0.3551, -0.3579, -0.1520],\n",
       "        [-0.2668, -0.1323, -0.0304,  ..., -0.4117, -0.3002,  0.0072],\n",
       "        [ 0.4472, -0.1382, -0.5853,  ...,  0.2730,  0.2051, -0.5639]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "share_conv=nn.Embedding(vocab_size, EMBED_SIZE).to(device)\n",
    "share_conv.weight.data.copy_(TEXT.vocab.vectors)\n",
    "# share_conv.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,embed_size,n_hid):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size = embed_size,\n",
    "                            hidden_size = n_hid,\n",
    "                            batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        input: \n",
    "            x = (batch, max_len, embed_dim)\n",
    "        output:\n",
    "            x = (batch, hidden_size)\n",
    "        \"\"\"\n",
    "        _, (h,c)=self.lstm(x) # h=(max_len, batch, n_hid)\n",
    "        return (h,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vae_classifier_2layer(nn.Module):\n",
    "    def __init__(self,n_hid):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(n_hid*2, n_hid)\n",
    "        self.fcmean = nn.Linear(n_hid, n_hid)\n",
    "        self.fcvar = nn.Linear(n_hid, n_hid)\n",
    "        self.ReLU=nn.ReLU()\n",
    "        self.lnorm=nn.LayerNorm(n_hid)\n",
    "\n",
    "    def forward(self, hidden):# h,c = (batch, hidden_size)\n",
    "        h, c = hidden\n",
    "        tmp = self.ReLU(self.lnorm(self.fc(torch.cat([h,c],dim=-1))))\n",
    "        mean = self.fcmean(tmp)\n",
    "        log_sigma_sq = self.fcvar(tmp)\n",
    "        return mean, log_sigma_sq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,embed_size,n_hid,max_len,vocab_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size = embed_size,\n",
    "                            hidden_size = n_hid,\n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(n_hid, vocab_size)\n",
    "        self.max_len=max_len\n",
    "        self.embedding=share_conv\n",
    "        \n",
    "    def forward(self, hidden, x, teacher):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            x_emb=(batch, hidden_size, embedding_size)\n",
    "        output:\n",
    "            logits = (batch, max_len, vocab_size)\n",
    "            sentence = (batch, max_len) : 中身はindex\n",
    "        \"\"\"\n",
    "        logits = torch.tensor([]).to(device)\n",
    "        sentence = torch.tensor([],dtype=torch.long).to(device)\n",
    "        \n",
    "        for i in range(self.max_len):\n",
    "            if teacher or i==0:\n",
    "                tmp = torch.unsqueeze(x[:,i,:],1) # tmp = (batch, 1, embed_dim)\n",
    "            else:\n",
    "                # word = (batch, 1, 1)\n",
    "                tmp = self.embedding(word) # tmp = (batch, 1, embed_dim)\n",
    "                x_emb = F.normalize(tmp)\n",
    "            x_input = tmp # x_input = (batch, 1, (embed_size + n_hid))\n",
    "            out, hidden = self.lstm(x_input, hidden)\n",
    "                # out = (batch, 1, n_hid)\n",
    "                # hidden = ((batch, 1, n_hid),(batch, 1, n_hid))\n",
    "            logit = self.fc(out) # logit = (batch, 1, vocab_size)\n",
    "            word = torch.argmax(logit, dim=2) # word = (batch, 1)\n",
    "\n",
    "            sentence = torch.cat([sentence, word],1)\n",
    "            logits = torch.cat([logits,logit],1)   \n",
    "        return logits, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self,maxlen):\n",
    "        super().__init__()\n",
    "        self.n_hid=LSTM_HIDDEN_SIZE\n",
    "        self.maxlen=maxlen\n",
    "        self.encoder=Encoder(embed_size=EMBED_SIZE,n_hid=self.n_hid)\n",
    "        self.vae_classifer=vae_classifier_2layer(n_hid=self.n_hid)\n",
    "        self.decoder=Decoder(embed_size=EMBED_SIZE,n_hid=self.n_hid,max_len=MAX_LEN,vocab_size=vocab_size)\n",
    "        self.embedding=share_conv\n",
    "        self.fc1 = nn.Linear(self.n_hid, self.n_hid)\n",
    "        self.fc2 = nn.Linear(self.n_hid, self.n_hid)\n",
    "        \n",
    "    def forward(self, x): # x=(batch, max_len)\n",
    "        x_emb=self.embedding(x)  # x_emb = (batch, maxlen, embed_dim)\n",
    "        x_emb = F.normalize(x_emb)\n",
    "        \n",
    "        hidden = self.encoder(x_emb) # h,c = (1, batch, hidden_size)\n",
    "        mean, log_sigma_sq=self.vae_classifer(hidden)\n",
    "        eps = torch.empty(len(x), self.n_hid).normal_(mean=0,std=1).to(device) # N(0, 1)\n",
    "        h = mean + eps * torch.sqrt(torch.exp(log_sigma_sq)) # H_dec = (1, batch, n_gan)\n",
    "        h,c=self.fc1(h), self.fc2(h)\n",
    "        logits, sentence = self.decoder((h,c), x_emb, teacher=True)\n",
    "        \n",
    "        # loss\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=1)\n",
    "        loss = 0\n",
    "        for i in range(self.maxlen-1):\n",
    "            # <SOS>を除くためindexをずらす\n",
    "            loss += criterion(torch.squeeze(logits[:,i,:]), torch.squeeze(x[:,i+1]))\n",
    "        loss/=(self.maxlen-1)\n",
    "        \n",
    "        # KL loss\n",
    "        # 標準正規分布と(μ,σ^2)正規分布を仮定しているので以下の計算式になる\n",
    "        # nn.klDivLossを使うと仮定が甘い\n",
    "        # kl_loss = Σ0.5Σ(μ^2+exp(ln(σ^2))-ln(σ^2)-1)を使う\n",
    "        kl_loss = torch.sum(0.5 * torch.sum((mean**2 + torch.exp(log_sigma_sq) - log_sigma_sq - 1),dim=1))\n",
    "        loss += epoch/AE_EPOCHS*kl_loss\n",
    "        \n",
    "        return loss, sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_to_sent(sents,j):\n",
    "    word_list=[]\n",
    "    for i in sents[j]:\n",
    "        if i==TEXT.vocab.stoi[\"<EOS>\"]:\n",
    "            break\n",
    "        if i!=TEXT.vocab.stoi[\"<PAD>\"]:\n",
    "            word_list.append(TEXT.vocab.itos[int(i)])\n",
    "    return word_list\n",
    "\n",
    "def write_out(url, origin_sents, syn_sents):\n",
    "    with open(url, \"w\") as f:\n",
    "        for j in range(len(syn_sents)):\n",
    "            f.write(\"input : \"+\" \".join(change_to_sent(origin_sents,j))+\"\\n\")\n",
    "            f.write(\"output: \"+\" \".join(change_to_sent(syn_sents,j))+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    auto_encoder.train()\n",
    "    epoch_loss = 0\n",
    "    count=0\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "        (x, x_l) = batch.Text\n",
    "            # xには文章のID表現が、x_lにはxの単語数が入る\n",
    "            # x=(batch, max_len)\n",
    "#         if len(x)!=BATCH_SIZE:break\n",
    "        optimizer.zero_grad()\n",
    "        loss, syn_sents=auto_encoder(x)        \n",
    "        with torch.autograd.detect_anomaly():\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss+=loss.item()\n",
    "        count+=1\n",
    "        \n",
    "    sample_x=x[0][1:]\n",
    "    source_sentence=' '.join([TEXT.vocab.itos[int(i)] for i in sample_x if i != 1])\n",
    "    gen_sentence=' '.join([TEXT.vocab.itos[int(i)] for i in syn_sents[0] if i != 1])\n",
    "\n",
    "    write_out(log_dir+\"{:03}.txt\".format(epoch), x[:,1:], syn_sents)\n",
    "\n",
    "    history_train.append(epoch_loss/count)\n",
    "    if (epoch+1) % REPORT_INTERVAL==0:\n",
    "        print(\"epoch: \"+str(epoch+1)+'/'+str(AE_EPOCHS)+' ')\n",
    "        print(\"training loss: \"+str(history_train[epoch]))\n",
    "#         print(\"kl_loss: \"+str(kl_loss))\n",
    "        print(\"source(train): \"+str(source_sentence))\n",
    "        print(\"result(train): \"+str(gen_sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start train...\n",
      "epoch: 1/100 \n",
      "training loss: 3.8837210144631547\n",
      "source(train): しかし 一 年 後 オリジナル 商品 に 力 を 入れる ため に 退社 し た <EOS>\n",
      "result(train): この この 日 に に に を し を 入れ よう に し し た <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 2/100 \n",
      "training loss: 18.854686817270974\n",
      "source(train): ただし 性能 は 従来 の もの より 低く 資金 面 から 開発 は 難航 し た <EOS>\n",
      "result(train): この この の 高く の もの として 不 なり 源 から 派生 し し し た <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 3/100 \n",
      "training loss: 2.6896356703787005\n",
      "source(train): モデナ の エステ 家 の 名門 の 家 の ライブラリ の 管理 を 行っ た <EOS>\n",
      "result(train): また の 教育 部 の ため の 一 の ため として 作曲 を 受け た <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 4/100 \n",
      "training loss: 2.3516829974966647\n",
      "source(train): ただし 検査 の ため の 回収 で あっ て 再 利用 さ れる こと は ない <EOS>\n",
      "result(train): また この の ため の 方法 で は て は 検討 さ れる こと は ない <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 5/100 \n",
      "training loss: 2.07323937363525\n",
      "source(train): 上述 し た もの より 良い 固定 係数 近似 アルゴリズム は 知ら れ て い ない <EOS>\n",
      "result(train): また し て もの は この 文字 係数 は 係数 は 大きく れ て い ない <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 6/100 \n",
      "training loss: 1.8478673454503725\n",
      "source(train): この 若者 達 が 政府 において も 次 の 時代 を 担っ て いく だろ う から <EOS>\n",
      "result(train): この 時期 が が 外国 の も 多く の よう を 取り上げ て いく だろ う <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 7/100 \n",
      "training loss: 1.6682418150580953\n",
      "source(train): 国内外 の 現代 美術 や 地域 に ゆかり の ある 作品 に 特色 が ある <EOS>\n",
      "result(train): また に 文化 文化 や 日本 に ゆかり の ある 作品 に 特色 が ある <EOS> <EOS> <EOS> あっ あっ\n",
      "epoch: 8/100 \n",
      "training loss: 1.5150754457283462\n",
      "source(train): フリック の ミス が 他 の アリ たち に バレ て しまっ た の だ <EOS>\n",
      "result(train): この の ミス の 決まっ の 候補 たち の バレ て しまっ た の だ <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 9/100 \n",
      "training loss: 1.392511292427709\n",
      "source(train): その 代わり 非 オランダ 語 読者 に は 読む の が 難しい もの に なっ た <EOS>\n",
      "result(train): また 教え子 に 詞 語 に に なっ 読む の が 難しい もの に なっ た <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 10/100 \n",
      "training loss: 1.2952185327105068\n",
      "source(train): この 事故 による 住民 や 列車 の 乗客 の 死傷 者 は 出 なかっ た <EOS>\n",
      "result(train): この 事故 は 旅客 の 事故 運行 乗客 の 死傷 者 は 出 なかっ た <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 11/100 \n",
      "training loss: 1.2072813926467763\n",
      "source(train): 古着 や 原宿 系 裏 原宿 系 の 女性 雑誌 として 認知 さ れ て い た <EOS>\n",
      "result(train): また や 原宿 系 の 原宿 系 の 雑誌 雑誌 として 認知 さ れ て い た <EOS> <EOS> <EOS>\n",
      "epoch: 12/100 \n",
      "training loss: 1.140957510042633\n",
      "source(train): ある 意味 で これ が 近代 的 な 発生 学 の 始まり と なっ て いる <EOS>\n",
      "result(train): また 大まか で 重要 が 近代 的 な 世界 の の 始まり と なっ て いる <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 13/100 \n",
      "training loss: 1.0843546451382737\n",
      "source(train): 全国 の 主要 な 新聞 社 の 記者 も 出席 し 黒人 系 メディア も 訪れ た <EOS>\n",
      "result(train): この の 住み 国 部署 社 の 年間 や 出席 権 新聞 不動産 メディア も 訪れ た <EOS> <EOS> <EOS>\n",
      "epoch: 14/100 \n",
      "training loss: 1.0393488671967712\n",
      "source(train): さらに 一部 の カウント を まとめ て 処理 する こと で も 処理 を 高速 化 <EOS>\n",
      "result(train): また 一部 の カウント を 加え たり 処理 する こと も も 合わせ を 高速 化 <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 15/100 \n",
      "training loss: 0.9876281104618878\n",
      "source(train): 製作 が 中断 し た ため 現物 は 存在 し ない が 復元 が 試み られ た <EOS>\n",
      "result(train): この は 中断 し た ため 現物 は 存在 し ない が 復元 が 試み られ た <EOS> <EOS> <EOS>\n",
      "epoch: 16/100 \n",
      "training loss: 0.9490005696179419\n",
      "source(train): 戦前 と 第 二 次 世界 大戦 中 の 亡命 政権 で 首相 を 務める <EOS>\n",
      "result(train): また と 二 二 次 世界 大戦 で で 激しい 政権 で 武 を 務める <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 17/100 \n",
      "training loss: 0.9204478956015647\n",
      "source(train): 種 内 の 変異 として は 果実 の 色 が 異なる もの を 品種 として 分ける <EOS>\n",
      "result(train): この 内 の 色 として は 果実 の 色 を 異なる もの を 品種 として 分ける <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 18/100 \n",
      "training loss: 0.8860536958944494\n",
      "source(train): 中山 キャンパス 八 千 代 キャンパス 共 に 図書 室 が 設置 さ れ て い た <EOS>\n",
      "result(train): この など 内 部 代 キャンパス 共 に 図書 室 が 設置 さ れ て い た <EOS> <EOS> <EOS>\n",
      "epoch: 19/100 \n",
      "training loss: 0.8658204579298014\n",
      "source(train): また 共 性 と 中性 の どちら か に 定まっ て い ない 名詞 も ある <EOS>\n",
      "result(train): また 性別 性 に 中性 の 存在 に に 問わ て い ない <EOS> も ある <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 20/100 \n",
      "training loss: 0.8412976530477785\n",
      "source(train): 裏庭 の さらに 後ろ は 台所 便所 倉庫 として 使わ れる 建物 が あっ た <EOS>\n",
      "result(train): また の さらに 後ろ は 台所 便所 倉庫 として 使わ れ 建物 マス あっ た <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 21/100 \n",
      "training loss: 0.8208242257897096\n",
      "source(train): その後 に それぞれ の 語 が どこ の 方言 で ある か を 説明 し て いる <EOS>\n",
      "result(train): この に まず の 言語 を それぞれ か か を ある か を 説明 し て いる <EOS> <EOS> <EOS>\n",
      "epoch: 22/100 \n",
      "training loss: 0.8030146716503975\n",
      "source(train): 長大 トンネル として は 九州 自動車 道 の 肥後 トンネル 同様 トンネル 内 に 県境 が ない <EOS>\n",
      "result(train): この トンネル の は 九州 自動車 道 の ほか トンネル 内 トンネル 内 に 県境 が ない <EOS> <EOS> <EOS>\n",
      "epoch: 23/100 \n",
      "training loss: 0.7871141518766531\n",
      "source(train): その後 画家 として で は なく 考古 学者 として 研究 に 従事 する よう に なっ た <EOS>\n",
      "result(train): この は として は は なく 考古 学者 として 有名 に 従事 する よう に なっ た <EOS> <EOS> <EOS>\n",
      "epoch: 24/100 \n",
      "training loss: 0.7638255801665534\n",
      "source(train): 戦前 は 馬 が 主要 な 交通 手段 で 交通 の 便 が 非常 に 悪かっ た <EOS>\n",
      "result(train): この は 馬 輸送 通る な 交通 手段 で 交通 の 便 が 非常 に 悪かっ た <EOS> <EOS> <EOS>\n",
      "epoch: 25/100 \n",
      "training loss: 0.7492971331088559\n",
      "source(train): その 時点 で は ダン は マトリックス の 真実 に は 気付い て い ない <EOS>\n",
      "result(train): また 時点 で は ダン は マトリックス の 真実 に は 気付い て い ない <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 26/100 \n",
      "training loss: 0.7468700774031283\n",
      "source(train): また この 時期 に 国民 自由党 の 組織 再 編成 において 重要 な 役割 を 果たし た <EOS>\n",
      "result(train): また 日本 時期 に 多く 自由党 の 参加 再 加盟 において 主要 な 役割 を 果たし た <EOS> <EOS> <EOS>\n",
      "epoch: 27/100 \n",
      "training loss: 0.7331623319545091\n",
      "source(train): かつて の フランク 族 の 領土 は 分割 さ れ 地主 に 分配 さ れ た <EOS>\n",
      "result(train): また の フランク 共和 の 権利 は 分割 さ れ 地主 に 分配 さ れ た <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 28/100 \n",
      "training loss: 0.7202726341870435\n",
      "source(train): ヴェネツィア 共和 国 は もはや 自力 で 防衛 する こと が でき なく なっ た <EOS>\n",
      "result(train): この 共和 国 で もはや する で 防衛 する こと が でき なく なっ た <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 29/100 \n",
      "training loss: 0.7127638832598994\n",
      "source(train): 報道 部 に 所属 し 記者 として も 取材 や 特集 を 制作 し て いる <EOS>\n",
      "result(train): また 部 に 所属 し て として も 活動 を 特集 を 制作 し て いる <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 30/100 \n",
      "training loss: 0.7136268033505593\n",
      "source(train): 関係 する 分子 の 種類 と 修復 の 機構 は 以下 の 条件 により 決まる <EOS>\n",
      "result(train): また する と の 場合 と する の 機構 は 以下 の 条件 により 決まる <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 31/100 \n",
      "training loss: 0.704690335729006\n",
      "source(train): 何 を 持っ て 下北 弁 に 応じ て い た の か 証明 が 無い <EOS>\n",
      "result(train): また を やっ て 下北 弁 に 応じ た い た と か 証明 が 無い <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 32/100 \n",
      "training loss: 0.7033025504818096\n",
      "source(train): また この よう な 教え 方 は 実用 に も そぐわない と 考え られ て いる <EOS>\n",
      "result(train): また この よう な 実用 方 は 実用 に も そぐわない と 考え られ て いる <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 33/100 \n",
      "training loss: 0.688488867540094\n",
      "source(train): 神 は 目 に 見え ない もの で あり 神 の 形 は 作ら れ なかっ た <EOS>\n",
      "result(train): また は 目 に 見え ない もの は あり テロ の 形 は 作ら れ なかっ た <EOS> <EOS> <EOS>\n",
      "epoch: 34/100 \n",
      "training loss: 0.6696919977250066\n",
      "source(train): 旧制 水戸 高校 を 経 て 東京大学 法学部 政治 学科 卒業 後 農林水産省 に 入省 <EOS>\n",
      "result(train): この 水戸 高校 を 経 て 東京大学 法学部 政治 学科 卒業 後 農林水産省 に 入省 <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 35/100 \n",
      "training loss: 0.666964105760415\n",
      "source(train): 文部 科学 委員 会 は 衆議院 のみ に 置か れる 常任 委員 会 で ある <EOS>\n",
      "result(train): また 科学 委員 会 は 衆議院 参議院 に 置か れる 常任 単一 会 で ある <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 36/100 \n",
      "training loss: 0.6627208859489024\n",
      "source(train): 本 種 は しばしば 接頭 辞 を 省い て 単に バス と も 略さ れる <EOS>\n",
      "result(train): また 結び は しばしば この 辞 を 省い て 単に バス と も 略さ れる <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 37/100 \n",
      "training loss: 0.6540285844675626\n",
      "source(train): ビッグバン 理論 の いくつ か の 問題 を 一挙 に 解決 する と さ れる <EOS>\n",
      "result(train): また で の いくつ か の 問題 と 一挙 に 解決 する と さ れる <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 38/100 \n",
      "training loss: 0.648073813685685\n",
      "source(train): 下記 以外 の 手続 について は 各人 が 全員 を 代表 し て 行う こと が できる <EOS>\n",
      "result(train): また の の 手続 について は 各人 が 全員 を 代表 し て 行う こと が できる <EOS> <EOS> <EOS>\n",
      "epoch: 39/100 \n",
      "training loss: 0.6537774869709723\n",
      "source(train): 以下 の 路線 バス が この 県道 の 一部 区間 を 運行 し て いる <EOS>\n",
      "result(train): また の 路線 バス が この 区間 沿線 区間 区間 を 運行 し て いる <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 40/100 \n",
      "training loss: 0.643859827947174\n",
      "source(train): こうした 国民 の 不満 を 吸収 し た の が エリツィン ら 急進 改革 派 で ある <EOS>\n",
      "result(train): また ため の 不満 を フランス語 し た の が エリツィン ら 急進 改革 派 で ある <EOS> <EOS> <EOS>\n",
      "epoch: 41/100 \n",
      "training loss: 0.638097041029388\n",
      "source(train): 市川 は 京都大 学 文学部 卒業 後 毎日新聞社 に 入社 記者 生活 を し て い た <EOS>\n",
      "result(train): また は 京都大 学 文学部 卒業 後 毎日新聞社 に 入社 記者 生活 を し て い た <EOS> <EOS> <EOS>\n",
      "epoch: 42/100 \n",
      "training loss: 0.6437258178631834\n",
      "source(train): こうして 工場 生産 の 原理 で ある 大量 生産 と 規格 化 が 実証 さ れ た <EOS>\n",
      "result(train): この 工場 の の 原理 で ある 大量 生産 化 規格 化 が 実証 さ れ た <EOS> <EOS> <EOS>\n",
      "epoch: 43/100 \n",
      "training loss: 0.6380033260016873\n",
      "source(train): 自分 で も まだ 本当 の 自分 が なん な の か はっきり わから ない <EOS>\n",
      "result(train): また 自身 も まだ 本当 の 自分 が なん な の か はっきり わから ない <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 44/100 \n",
      "training loss: 0.6253975844853439\n",
      "source(train): 政界 引退 後 も 日本 酒造 組合 中央 会 会長 など を 歴任 し た <EOS>\n",
      "result(train): また 引退 後 も 日本 酒造 組合 中央 会 会長 など を 歴任 し た <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 45/100 \n",
      "training loss: 0.6309910214002614\n",
      "source(train): 座布団 運び は 基本 的 に 若手 の 落語 家 が 務め て い た <EOS>\n",
      "result(train): また 運び は 当時 的 に 若手 の 落語 家 が 務め て い た <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 46/100 \n",
      "training loss: 0.6248027790519312\n",
      "source(train): 制服 は 日本 の 各 学校 を 視察 し て 良い 所 を 取り入れ た <EOS>\n",
      "result(train): また は それぞれ 各 学校 学校 を 視察 し て 良い 所 を 取り入れ た <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 47/100 \n",
      "training loss: 0.6299207916530821\n",
      "source(train): さまざま な 分野 で 使わ れ て おり 場合 によって 意味 が 異なる こと も ある <EOS>\n",
      "result(train): また な 意味 で 使わ れ て おり 国府 も 意味 が 異なる こと も ある <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 48/100 \n",
      "training loss: 0.6368225797699665\n",
      "source(train): 当初 国王 候補 者 は 部族 大公 の 有力 者 から 選ば れ て い た <EOS>\n",
      "result(train): また は 候補 者 は 王 大公 の 王 者 から 選ば れ て い た <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 49/100 \n",
      "training loss: 0.6071342252661623\n",
      "source(train): 特に スタミナ は 女子 代表 全 カテゴリ を通して も トップクラス と 言わ れ て いる <EOS>\n",
      "result(train): また スタミナ は 女子 代表 全 カテゴリ を通して も トップクラス と 言わ れ て いる <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 50/100 \n",
      "training loss: 0.6057780179651043\n",
      "source(train): ジョン は 妹 たち に 自分 の 出自 を 話し サンサ から ティリオン に も 伝わる <EOS>\n",
      "result(train): また は 妹 に に 出自 の 出自 を 話し サンサ から ティリオン に も 伝わる <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 51/100 \n",
      "training loss: 0.6160341354990226\n",
      "source(train): グラハム 市 に ある アメリカ合衆国 国家 歴史 登録 財 として は 次 の もの が ある <EOS>\n",
      "result(train): また 市 に ある 地域 国家 歴史 登録 財 として は 次 の もの が ある <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 52/100 \n",
      "training loss: 0.6112864086633492\n",
      "source(train): しかし この 作品 は 本来 の 仮面 劇 と いう より は バレエ に 近い <EOS>\n",
      "result(train): また 現在 作品 は 本来 の 仮面 劇 と いう より は マンハイム に 近い <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 53/100 \n",
      "training loss: 0.6203864422862325\n",
      "source(train): 在任 中 は レスリング と ボクシング の 両方 で 海軍 の チャンピオン に なっ た という <EOS>\n",
      "result(train): また 中 は レスリング と 海軍 の 両方 に 海軍 の チャンピオン に なっ た という <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 54/100 \n",
      "training loss: 0.6021446570788624\n",
      "source(train): 現在 は 白石 市 図書館 前 に 移転 し 文化財 保存 庫 と なっ て いる <EOS>\n",
      "result(train): また は 白石 市 発足 前 に 現存 し 保存 保存 庫 と なっ て いる <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 55/100 \n",
      "training loss: 0.6012915082085714\n",
      "source(train): この よう な 道徳 教育 は 学童 期 が 最適 で ある と し た <EOS>\n",
      "result(train): また よう な 道徳 化 で 学童 期 が 最適 で ある と し た <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 56/100 \n",
      "training loss: 0.5977102942989486\n",
      "source(train): また 国土 面積 や 人口 の 小さな ミニ 国家 に 分類 さ れる 国 が 多い <EOS>\n",
      "result(train): また マリアナ 面積 や 人口 の 小さな ミニ 国家 に 分類 さ れる 国 が 多い <EOS> <EOS> 種 <EOS>\n",
      "epoch: 57/100 \n",
      "training loss: 0.5986808747606321\n",
      "source(train): 静岡 県 告示 に 基づく 起 終点 および 重要 な 経過 地 は 以下 の とおり <EOS>\n",
      "result(train): また 県 告示 に 基づく 起 終点 および 重要 な 経過 地 は 以下 の とおり <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 58/100 \n",
      "training loss: 0.6097842512302222\n",
      "source(train): 教員 として 働く 機会 を 得 て も その 多く は 非常勤 講師 で ある <EOS>\n",
      "result(train): また として は 機会 が 多く て も その 多く は 非常勤 講師 として ある <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 59/100 \n",
      "training loss: 0.5994567157843274\n",
      "source(train): 地域 がん 登録 は 主 に 都道府県 を 主体 として 運営 さ れ て いる <EOS>\n",
      "result(train): また がん 登録 は 主 に 都道府県 を 主体 として 運営 さ れ て いる <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 60/100 \n",
      "training loss: 0.5959129223206757\n",
      "source(train): 自ら を トルコ 正規 軍 より 上 に 立つ 存在 で ある と 主張 し た <EOS>\n",
      "result(train): また を 禁止 系 軍 より 上 に 立つ と で ある と 主張 し た <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 61/100 \n",
      "training loss: 0.6204641488339674\n",
      "source(train): 近い うち に ロシア で の 公演 が 行なわ れる こと が 発表 さ れ た <EOS>\n",
      "result(train): また うち に フランス で 初めて 公演 が 行なわ れる こと が 発表 さ れ た <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 62/100 \n",
      "training loss: 0.589673928103978\n",
      "source(train): 高 さ は 建築 物 本体 を 基準 と し アンテナ など の 高 さ は 除く <EOS>\n",
      "result(train): また さ は 建築 物 本体 を 基準 と し アンテナ など の 高 さ は 除く <EOS> <EOS> <EOS>\n",
      "epoch: 63/100 \n",
      "training loss: 0.5804375680444412\n",
      "source(train): 元首 は 人民 に対して 支配 者 で ある と 同時に 教育 者 で なけれ ば なら ぬ <EOS>\n",
      "result(train): また は 人民 に対して 支配 者 で ある と 同時に 決して 者 で なけれ ば なら ぬ <EOS> <EOS> <EOS>\n",
      "epoch: 64/100 \n",
      "training loss: 0.5883346326182447\n",
      "source(train): 小 選挙 区 で は 再び 古本 に 敗れ た が 比例 復活 で 再選 <EOS>\n",
      "result(train): また 選挙 区 で は 再び 古本 に 敗れ た が 比例 復活 で 再選 <EOS> で <EOS> で で\n",
      "epoch: 65/100 \n",
      "training loss: 0.6082886389679256\n",
      "source(train): メソッド は オブジェクト を インスタンス 化 する ため に 使わ れ た クラス の オブジェクト を 返す <EOS>\n",
      "result(train): また は オブジェクト を インスタンス 化 する ため に 使わ れ た クラス の オブジェクト を 返す <EOS> を <EOS>\n",
      "epoch: 66/100 \n",
      "training loss: 0.598992957385676\n",
      "source(train): その後 最高 裁判所 へ の 上告 は 行わ れ ず 判決 が 確定 し た <EOS>\n",
      "result(train): また 最高 裁判所 へ の 上告 は 行わ れ かね 判決 が 確定 し た <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 67/100 \n",
      "training loss: 0.5824556859629336\n",
      "source(train): オピオイド を 使用 する か どう か について も 方向 性 が 分かれ て いる <EOS>\n",
      "result(train): また を 使用 する か どう か について も 基本 性 が 分かれ て いる <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 68/100 \n",
      "training loss: 0.5982793639748942\n",
      "source(train): 聖書 より 新しい 時代 の ヘブライ 語 として は 死 海 文書 の 言語 が ある <EOS>\n",
      "result(train): この より 現代 時代 の ヘブライ 語 の は 死 海 文書 の 光 が ある <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 69/100 \n",
      "training loss: 0.5791205914557118\n",
      "source(train): イギリス と フランス で は ろうそく の ため の 手工業 ギルド も 作ら れ て いる <EOS>\n",
      "result(train): また と 中国人 で は ろうそく の 教師 の 手工業 ギルド も 作ら れ て いる <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 70/100 \n",
      "training loss: 0.584150791928829\n",
      "source(train): 研究 によって 特定 の キャリア タンパク質 と 特定 の 病気 が 関連付け られ て いる <EOS>\n",
      "result(train): また 用 特定 の キャリア タンパク質 と 特定 の 病気 が 関連付け られ て いる <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 71/100 \n",
      "training loss: 0.5803513071929772\n",
      "source(train): 自国 のみ で も 多種 の 航空機 を 製造 し て 実用 化 し て いる <EOS>\n",
      "result(train): また のみ で も 航空機 の 航空機 を 製造 し て 実用 化 し て いる <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 72/100 \n",
      "training loss: 0.5897562511766593\n",
      "source(train): 第 二 次 上田 合戦 後 に は 上田 城 の 受取 役 を 果たし て いる <EOS>\n",
      "result(train): また 二 次 上田 合戦 後 に は 上田 城 の 受取 役 を 果たし て いる <EOS> <EOS> <EOS>\n",
      "epoch: 73/100 \n",
      "training loss: 0.5886391892925492\n",
      "source(train): しかし この 時点 で 建設 費用 で の 問題 が 一部 未解決 と なっ て い た <EOS>\n",
      "result(train): また この 時点 で 建設 費用 で の 問題 が 一部 未解決 と なっ て い た <EOS> <EOS> <EOS>\n",
      "epoch: 74/100 \n",
      "training loss: 0.5740092318254392\n",
      "source(train): 登場 は する もの の 起動 は し て おら ず 間接 的 な 対決 と なる <EOS>\n",
      "result(train): また し する もの の 起動 は し て おら ず 間接 的 な 対決 と なる <EOS> <EOS> <EOS>\n",
      "epoch: 75/100 \n",
      "training loss: 0.5768227185492173\n",
      "source(train): 市街地 は 主 に 栃木 駅 北側 から 新 栃木 駅 西側 にかけて 集中 し て いる <EOS>\n",
      "result(train): また は 主 に 栃木 駅 北側 から 同 栃木 駅 西側 にかけて 集中 し て いる <EOS> <EOS> <EOS>\n",
      "epoch: 76/100 \n",
      "training loss: 0.5739260372941289\n",
      "source(train): その 異常 な まで の タフ さ の 秘密 は 少年 時代 の 過去 に ある <EOS>\n",
      "result(train): また 異常 な まで の タフ さ の 秘密 インド インド 時代 の 過去 に ある <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 77/100 \n",
      "training loss: 0.5829677561388769\n",
      "source(train): それ によって マケドニア 地域 は 不安定 な 混迷 の 地 と なっ て いっ た <EOS>\n",
      "result(train): また によって マケドニア 地域 は インディアン な 混迷 の 地 と なっ て いっ た <EOS> <EOS> <EOS> も <EOS>\n",
      "epoch: 78/100 \n",
      "training loss: 0.5839609959769968\n",
      "source(train): さらに 航空機 など で 多用 さ れ て いる ジュラルミン も 使用 さ れ た <EOS>\n",
      "result(train): また 航空機 など で 多用 さ れ て いる ジュラルミン も 使用 さ れ た <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 79/100 \n",
      "training loss: 0.5773308162691975\n",
      "source(train): その 代わり フェニキア 人 は イベリア 人 に 数字 や アルファベット を 伝え た の で ある <EOS>\n",
      "result(train): また 代わり フェニキア 人 に イベリア 人 や 数字 や アルファベット を 伝え た の で ある <EOS> <EOS> <EOS>\n",
      "epoch: 80/100 \n",
      "training loss: 0.5680827477691898\n",
      "source(train): パナマ 運河 や イギリス の テムズ 川 と 運河 は ロック 方式 に なっ て いる <EOS>\n",
      "result(train): また 運河 や イギリス の テムズ 州 は 考え は ロック 方式 に なっ て いる <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 81/100 \n",
      "training loss: 0.5731133221888487\n",
      "source(train): 当初 運営 は 阪神電気鉄道 の 子会社 で ある 株式会社 神戸 ホテル 阪神 が 担っ た <EOS>\n",
      "result(train): また ソ連 は 阪神電気鉄道 の 子会社 で ある 株式会社 神戸 ホテル 阪神 が 担っ た <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 82/100 \n",
      "training loss: 0.5746531100671418\n",
      "source(train): また 他社 が 模型 化 し ない 形式 も 積極 的 に 模型 化 し て いる <EOS>\n",
      "result(train): また 他社 も 模型 化 し ない 形式 も 積極 的 に 模型 化 し て いる <EOS> 上京 <EOS>\n",
      "epoch: 83/100 \n",
      "training loss: 0.5836948339941883\n",
      "source(train): そこで 車体 を 軽量 化 し た 戦車 の 開発 が 行わ れる こと と なっ た <EOS>\n",
      "result(train): また 車体 の 軽量 化 し た 戦車 の 開発 が 行わ れる こと と なっ た <EOS> <EOS> <EOS>\n",
      "epoch: 84/100 \n",
      "training loss: 0.6006836545785452\n",
      "source(train): この こと により コントラスト について の 情報 を 伝達 する こと が 可能 だ から で ある <EOS>\n",
      "result(train): この こと により 視点 について の 情報 が 伝達 する こと が 可能 で から で ある <EOS> <EOS> <EOS>\n",
      "epoch: 85/100 \n",
      "training loss: 0.5766990110796731\n",
      "source(train): 人間 において も 吸気 音 は ゲル マン 語 派 など で 用い られ て いる <EOS>\n",
      "result(train): また で も 吸気 音 は ゲル マン 語 など など で 用い られ て いる <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 86/100 \n",
      "training loss: 0.5817058583561616\n",
      "source(train): 中 全 音律 で は 異名 同音 的 音程 は 異なる 大き さ を 持つ <EOS>\n",
      "result(train): また 全 音律 で は 定義 同音 的 音程 は 異なる 大き さ を 持つ <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 87/100 \n",
      "training loss: 0.5740557730267053\n",
      "source(train): また 地域 の 小学校 中学校 高等 学校 と の 交流 活動 を 重視 し て いる <EOS>\n",
      "result(train): また 各地 の 商業 中学校 高等 学校 の 研究 関連 活動 を 重視 し て いる <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 88/100 \n",
      "training loss: 0.571323780200321\n",
      "source(train): 独自 の アニミズム 論 を 構築 し た こと で 広く 知ら れ て いる <EOS>\n",
      "result(train): この の アニミズム 論 を 構築 し た こと で 広く 知ら れ て いる <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 89/100 \n",
      "training loss: 0.5679863366339434\n",
      "source(train): ボンネット と ボディ の 間 の 整形 は 初期 に は 行わ れ なかっ た <EOS>\n",
      "result(train): この と ボディ の 間 の 整形 は 初期 に は 行わ れ なかっ た <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 90/100 \n",
      "training loss: 0.5829926940446111\n",
      "source(train): また アルバニア 語 や アラン 諸島 で 話さ れる アイルランド 語 の 方言 を 研究 し た <EOS>\n",
      "result(train): この アルバニア 語 や アラン 諸島 で ラテン れる アイルランド 語 の 方言 を 研究 し た <EOS> <EOS> <EOS>\n",
      "epoch: 91/100 \n",
      "training loss: 0.5701651509220251\n",
      "source(train): 全 区間 が 下記 の とおり 高速 自動車 国道 の 路線 として 指定 さ れ て いる <EOS>\n",
      "result(train): また 区間 が 下記 の とおり 高速 自動車 国道 の 路線 として 指定 さ れ て いる <EOS> <EOS> <EOS>\n",
      "epoch: 92/100 \n",
      "training loss: 0.5630296365360097\n",
      "source(train): 母語 で ある 中国 語 の 他 に 第 二 言語 として 英語 を 使用 できる <EOS>\n",
      "result(train): また で ある ウズベク 語 の 他 に 第 二 言語 として 英語 を 使用 できる <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 93/100 \n",
      "training loss: 0.5811017914534722\n",
      "source(train): その ほか 筋肉 内 注射 静脈 内 注射 で 使用 さ れる 国 も ある <EOS>\n",
      "result(train): また ほか 筋肉 内 注射 静脈 内 注射 で 使用 さ れる 国 も ある <EOS> よ <EOS> <EOS> <EOS>\n",
      "epoch: 94/100 \n",
      "training loss: 0.5672584062662589\n",
      "source(train): 信長 は 将軍 に これ を 奏 達し 信長 自身 も 命令 を 発し て いる <EOS>\n",
      "result(train): また は 将軍 を これ を 奏 達し 信長 自身 も 命令 を 発し て いる <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 95/100 \n",
      "training loss: 0.57438993319143\n",
      "source(train): 映画 の 多く は ディック の 原題 を そのまま 題名 に し て い ない <EOS>\n",
      "result(train): また の 大半 は ディック の 原題 を そのまま 題名 に し て い ない <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 96/100 \n",
      "training loss: 0.5692012504676656\n",
      "source(train): また この 改修 により オリジナル に 比べ 全 高 と 重量 が 増し て いる <EOS>\n",
      "result(train): また この 装置 により オリジナル に 比べ デザイン 高 さ 重量 が 増し て いる <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 97/100 \n",
      "training loss: 0.569489986534185\n",
      "source(train): それら の 中 で 現在 その 中心 と なっ て いる の が 同書 で ある <EOS>\n",
      "result(train): また の 中 で 現在 と 中心 と なっ て いる の が 同書 で ある <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 98/100 \n",
      "training loss: 0.5758306820057938\n",
      "source(train): 等 速 直線 運動 において は 運動 量 と 角 運動 量 は ともに 保存 する <EOS>\n",
      "result(train): また 速 直線 運動 において は 平面 量 と 角 運動 量 は ともに 保存 する <EOS> 率 長 <EOS>\n",
      "epoch: 99/100 \n",
      "training loss: 0.5832815128108465\n",
      "source(train): この ため 制定 後 すぐ により 高速 な 方式 の 提案 が 乱立 し た <EOS>\n",
      "result(train): また ため 制定 後 すぐ により 高速 な 作戦 の 提案 が 乱立 し た <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 100/100 \n",
      "training loss: 0.5684574073193521\n",
      "source(train): 世界 ジュニア で は 初戦 で ロシア の 選手 に 技 あり で 敗れ た <EOS>\n",
      "result(train): また 大戦 で は 初戦 で ロシア の 選手 で も あり で 敗れ た <EOS> <EOS> <EOS> で で\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "学習を始める\n",
    "'''\n",
    "print(\"start train...\")\n",
    "auto_encoder=AutoEncoder(maxlen=MAX_LEN)\n",
    "auto_encoder.to(device)\n",
    "optimizer = optim.Adam(auto_encoder.parameters(), lr=ae_lr)\n",
    "history_train=[]\n",
    "history_eval=[]\n",
    "\n",
    "for epoch in range(AE_EPOCHS):\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [i for i in range(len(history_train))]\n",
    "plt.plot(x, history_train, label=\"train\")\n",
    "# plt.plot(x, history_eval, label=\"eval\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttext確認用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nihongo_id=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「その」の似ている単語\n",
      "この:tensor(0.7487, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "の:tensor(0.6892, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "が:tensor(0.6426, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "た:tensor(0.6121, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "それ:tensor(0.6105, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "これ:tensor(0.6037, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "も:tensor(0.5796, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "を:tensor(0.5789, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "しかし:tensor(0.5774, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "に:tensor(0.5679, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "nihongo_id+=1\n",
    "norm_list=[]\n",
    "word=TEXT.vocab.itos[nihongo_id]\n",
    "nihongo_vector=share_conv.weight[nihongo_id]\n",
    "for i, val in enumerate(share_conv.weight):\n",
    "    cos=torch.nn.CosineSimilarity(dim=0)\n",
    "    diff=cos(nihongo_vector,val)\n",
    "    norm=torch.norm(diff)\n",
    "    norm_list.append((norm,i))\n",
    "norm_list.sort(reverse=True)\n",
    "print(\"「\"+word+\"」の似ている単語\")\n",
    "for i in range(1,11):\n",
    "    val=norm_list[i][0]\n",
    "    id_=norm_list[i][1]\n",
    "    print(TEXT.vocab.itos[id_]+\":\"+str(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86188\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/foruse/\"+\"_\".join([\"train\",data_source,feature])+'.tsv') as f:\n",
    "    c=0\n",
    "    for i in f:\n",
    "        c+=1\n",
    "print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36-yasuda",
   "language": "python",
   "name": "py36-yasuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
