{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data, datasets\n",
    "from torchtext.vocab import FastText\n",
    "\n",
    "import os\n",
    "import math\n",
    "import MeCab\n",
    "import shutil\n",
    "\n",
    "device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 1\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "mecab = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "# BLEU\n",
    "\n",
    "from Util.selfbleu import CalcSelfBLEU\n",
    "from Util.bleu import calc_all_bleu\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPER PARAMETER\n",
    "BATCH_SIZE=100\n",
    "EMBED_SIZE=300\n",
    "LSTM_HIDDEN_SIZE=300\n",
    "AE_EPOCHS=100\n",
    "REPORT_INTERVAL=1\n",
    "ae_lr=1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL NAME\n",
    "data_source=\"wiki\"\n",
    "feature=\"only_ja_small\"\n",
    "feature=\"only_ja_sample\"\n",
    "# feature=\"all\"\n",
    "\n",
    "log_dir='./log/{}/'.format(\"_\".join([\"train\",data_source,feature]))\n",
    "if os.path.isdir(log_dir):\n",
    "    shutil.rmtree(log_dir)\n",
    "os.mkdir(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_source==\"wiki\" or data_source==\"orphans\":\n",
    "    MAX_LEN=20 #paddingをバッチにかかわらず固定長にする\n",
    "    TEXT = data.Field(sequential=True, lower=True, include_lengths=True, init_token=\"<SOS>\",eos_token=\"<EOS>\", fix_length=MAX_LEN, batch_first=True)\n",
    "    train, val, test = data.TabularDataset.splits(\n",
    "        path='./data/foruse', train=\"_\".join([\"train\",data_source,feature])+'.tsv',\n",
    "        validation=\"_\".join([\"eval\",data_source,feature])+'.tsv',test=\"_\".join([\"test\",data_source,feature])+'.tsv', \n",
    "        format='tsv',\n",
    "        fields=[('Text', TEXT),])\n",
    "    TEXT.build_vocab(train, vectors=FastText(language=\"ja\"))\n",
    "    train_iter, val_iter, test_iter = data.Iterator.splits((train, val, test), batch_sizes=(BATCH_SIZE, BATCH_SIZE, BATCH_SIZE),sort = False, device=torch.device(device))\n",
    "    vocab_size=TEXT.vocab.vectors.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_conv=nn.Embedding(vocab_size, EMBED_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,embed_size,n_hid):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size = embed_size,\n",
    "                            hidden_size = n_hid,\n",
    "                            batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        input: \n",
    "            x = (batch, max_len, embed_dim)\n",
    "        output:\n",
    "            x = (batch, hidden_size)\n",
    "        \"\"\"\n",
    "        _, (h,c)=self.lstm(x) # h=(max_len, batch, n_hid)\n",
    "        return (h,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vae_classifier_2layer(nn.Module):\n",
    "    def __init__(self,n_hid):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(n_hid*2, n_hid)\n",
    "        self.fcmean = nn.Linear(n_hid, n_hid)\n",
    "        self.fcvar = nn.Linear(n_hid, n_hid)\n",
    "        self.ReLU=nn.ReLU()\n",
    "        self.lnorm=nn.LayerNorm(n_hid)\n",
    "\n",
    "    def forward(self, hidden):# h,c = (batch, hidden_size)\n",
    "        h, c = hidden\n",
    "        tmp = self.ReLU(self.lnorm(self.fc(torch.cat([h,c],dim=-1))))\n",
    "        mean = self.fcmean(tmp)\n",
    "        log_sigma_sq = self.fcvar(tmp)\n",
    "        return mean, log_sigma_sq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,embed_size,n_hid,max_len,vocab_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size = embed_size,\n",
    "                            hidden_size = n_hid,\n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(n_hid, vocab_size)\n",
    "        self.max_len=max_len\n",
    "        self.embedding=share_conv\n",
    "        \n",
    "    def forward(self, hidden, x, teacher):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            x_emb=(batch, hidden_size, embedding_size)\n",
    "        output:\n",
    "            logits = (batch, max_len, vocab_size)\n",
    "            sentence = (batch, max_len) : 中身はindex\n",
    "        \"\"\"\n",
    "        logits = torch.tensor([]).to(device)\n",
    "        sentence = torch.tensor([],dtype=torch.long).to(device)\n",
    "        \n",
    "        for i in range(self.max_len):\n",
    "            if teacher or i==0:\n",
    "                tmp = torch.unsqueeze(x[:,i,:],1) # tmp = (batch, 1, embed_dim)\n",
    "            else:\n",
    "                # word = (batch, 1, 1)\n",
    "                tmp = self.embedding(word) # tmp = (batch, 1, embed_dim)\n",
    "                x_emb = F.normalize(tmp)\n",
    "            x_input = tmp # x_input = (batch, 1, (embed_size + n_hid))\n",
    "            out, hidden = self.lstm(x_input, hidden)\n",
    "                # out = (batch, 1, n_hid)\n",
    "                # hidden = ((batch, 1, n_hid),(batch, 1, n_hid))\n",
    "            logit = self.fc(out) # logit = (batch, 1, vocab_size)\n",
    "            word = torch.argmax(logit, dim=2) # word = (batch, 1)\n",
    "\n",
    "            sentence = torch.cat([sentence, word],1)\n",
    "            logits = torch.cat([logits,logit],1)   \n",
    "        return logits, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self,maxlen):\n",
    "        super().__init__()\n",
    "        self.n_hid=LSTM_HIDDEN_SIZE\n",
    "        self.maxlen=maxlen\n",
    "        self.encoder=Encoder(embed_size=EMBED_SIZE,n_hid=self.n_hid)\n",
    "        self.vae_classifer=vae_classifier_2layer(n_hid=self.n_hid)\n",
    "        self.decoder=Decoder(embed_size=EMBED_SIZE,n_hid=self.n_hid,max_len=MAX_LEN,vocab_size=vocab_size)\n",
    "        self.embedding=share_conv\n",
    "        self.fc1 = nn.Linear(self.n_hid, self.n_hid)\n",
    "        self.fc2 = nn.Linear(self.n_hid, self.n_hid)\n",
    "        \n",
    "    def forward(self, x): # x=(batch, max_len)\n",
    "        x_emb=self.embedding(x)  # x_emb = (batch, maxlen, embed_dim)\n",
    "        x_emb = F.normalize(x_emb)\n",
    "        \n",
    "        hidden = self.encoder(x_emb) # h,c = (1, batch, hidden_size)\n",
    "        mean, log_sigma_sq=self.vae_classifer(hidden)\n",
    "        eps = torch.empty(len(x), self.n_hid).normal_(mean=0,std=1).to(device) # N(0, 1)\n",
    "        h = mean + eps * torch.sqrt(torch.exp(log_sigma_sq)) # H_dec = (1, batch, n_gan)\n",
    "        h,c=self.fc1(h), self.fc2(h)\n",
    "        logits, sentence = self.decoder((h,c), x_emb, teacher=True)\n",
    "        \n",
    "        # loss\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=1)\n",
    "        loss = 0\n",
    "        for i in range(self.maxlen-1):\n",
    "            # <SOS>を除くためindexをずらす\n",
    "            loss += criterion(torch.squeeze(logits[:,i,:]), torch.squeeze(x[:,i+1]))\n",
    "        loss/=(self.maxlen-1)\n",
    "        \n",
    "        # KL loss\n",
    "        # 標準正規分布と(μ,σ^2)正規分布を仮定しているので以下の計算式になる\n",
    "        # nn.klDivLossを使うと仮定が甘い\n",
    "        # kl_loss = Σ0.5Σ(μ^2+exp(ln(σ^2))-ln(σ^2)-1)を使う\n",
    "        kl_loss = torch.sum(0.5 * torch.sum((mean**2 + torch.exp(log_sigma_sq) - log_sigma_sq - 1),dim=1))\n",
    "        loss += epoch/AE_EPOCHS*kl_loss\n",
    "        \n",
    "        return loss, sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_to_sent(sents,j):\n",
    "    word_list=[]\n",
    "    for i in sents[j]:\n",
    "        if i==TEXT.vocab.stoi[\"<EOS>\"]:\n",
    "            break\n",
    "        if i!=TEXT.vocab.stoi[\"<PAD>\"]:\n",
    "            word_list.append(TEXT.vocab.itos[int(i)])\n",
    "    return word_list\n",
    "\n",
    "def write_out(url, origin_sents, syn_sents):\n",
    "    with open(url, \"a\") as f:\n",
    "        for j in range(len(syn_sents)):\n",
    "            f.write(\"input : \"+\" \".join(change_to_sent(origin_sents,j))+\"\\n\")\n",
    "            f.write(\"output: \"+\" \".join(change_to_sent(syn_sents,j))+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    auto_encoder.train()\n",
    "    epoch_loss = 0\n",
    "    count=0\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "        (x, x_l) = batch.Text\n",
    "            # xには文章のID表現が、x_lにはxの単語数が入る\n",
    "            # x=(batch, max_len)\n",
    "        if len(x)!=BATCH_SIZE:break\n",
    "        optimizer.zero_grad()\n",
    "        loss, syn_sents=auto_encoder(x)\n",
    "        with torch.autograd.detect_anomaly():\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss+=loss.item()\n",
    "        count+=1\n",
    "        \n",
    "    sample_x=x[0][1:]\n",
    "    source_sentence=' '.join([TEXT.vocab.itos[int(i)] for i in sample_x if i != 1])\n",
    "    gen_sentence=' '.join([TEXT.vocab.itos[int(i)] for i in syn_sents[0] if i != 1])\n",
    "\n",
    "    write_out(log_dir+\"{:03}.txt\".format(epoch), x[:,1:], syn_sents)\n",
    "\n",
    "    history_train.append(epoch_loss/count)\n",
    "    if (epoch+1) % REPORT_INTERVAL==0:\n",
    "        print(\"epoch: \"+str(epoch+1)+'/'+str(AE_EPOCHS)+' ')\n",
    "        print(\"training loss: \"+str(history_train[epoch]))\n",
    "#         print(\"kl_loss: \"+str(kl_loss))\n",
    "        print(\"source(train): \"+str(source_sentence))\n",
    "        print(\"result(train): \"+str(gen_sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start train...\n",
      "epoch: 1/100 \n",
      "training loss: 5.864137172698975\n",
      "source(train): 逆 に マルタ は 長い 間 アフリカ に 属する 島 と 受け止め られ て い た <EOS>\n",
      "result(train): 性 性 あり 予算 その後 し いう これ 石材 街 語族 また また 述べる 分け 今日 し し 今日 今日\n",
      "epoch: 2/100 \n",
      "training loss: 397.9302978515625\n",
      "source(train): 数 多く の 病院 が パリ に 設置 さ れ て いる <EOS>\n",
      "result(train): この この は は は <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 3/100 \n",
      "training loss: 140.99566650390625\n",
      "source(train): 自立 語 は 活用 の ない もの と 活用 の ある もの と に 分け られる <EOS>\n",
      "result(train): この の の <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 4/100 \n",
      "training loss: 490.1822814941406\n",
      "source(train): この ギャル 文字 を 練習 する ため の 本 も 現れ た <EOS>\n",
      "result(train): 尊敬 手法 変わっ 発音 母語 母語 <EOS> 属する <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 5/100 \n",
      "training loss: 274.2007751464844\n",
      "source(train): 日本 で 生まれ 育っ た ほとんど の 人 は 日本語 を 母語 と する <EOS>\n",
      "result(train): <EOS> <EOS> ある <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> の の の の\n",
      "epoch: 6/100 \n",
      "training loss: 167.2931671142578\n",
      "source(train): これ によって 初めて 日本語 を 自由 に 記す こと が 可能 に なっ た <EOS>\n",
      "result(train): この は は は は の の の の の の の の <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 7/100 \n",
      "training loss: 195.38124084472656\n",
      "source(train): 自立 語 は 活用 の ない もの と 活用 の ある もの と に 分け られる <EOS>\n",
      "result(train): あっ <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 8/100 \n",
      "training loss: 191.14199829101562\n",
      "source(train): この ギャル 文字 を 練習 する ため の 本 も 現れ た <EOS>\n",
      "result(train): <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 9/100 \n",
      "training loss: 123.7173080444336\n",
      "source(train): 文 は 目的 や 場面 など に 応じ て さまざま な 異なっ た 様式 を 採る <EOS>\n",
      "result(train): <EOS> な 現在 と と と <EOS> と <EOS> いる <EOS> と <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 10/100 \n",
      "training loss: 112.40652465820312\n",
      "source(train): パリ 市内 は 現在 も 大学 の 中心 地 で あり 続け て いる <EOS>\n",
      "result(train): 人 <EOS> ある 現在 <EOS> ある <EOS> <EOS> <EOS> <EOS> ある <EOS> <EOS> いる <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 11/100 \n",
      "training loss: 104.27486419677734\n",
      "source(train): しかし 方言 によって は 今 も 開 合 の 区別 が 残っ て いる もの も ある <EOS>\n",
      "result(train): 少ない 雨量 によって も 現在 と ある ある と <EOS> と ある <EOS> いる <EOS> <EOS> ある <EOS> <EOS> <EOS>\n",
      "epoch: 12/100 \n",
      "training loss: 79.95535278320312\n",
      "source(train): この うち とりわけ 目 を 引く の は 形容詞 の 少な さ で ある <EOS>\n",
      "result(train): 他 海水 が <EOS> と する <EOS> <EOS> ある <EOS> <EOS> も <EOS> ある <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 13/100 \n",
      "training loss: 82.34085083007812\n",
      "source(train): 地球 以外 に 生命 が 発見 さ れ た 事例 は 記録 さ れ て い ない <EOS>\n",
      "result(train): この も も て <EOS> ある <EOS> れる て <EOS> <EOS> <EOS> <EOS> れる て いる <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 14/100 \n",
      "training loss: 72.95103454589844\n",
      "source(train): どれ も 一長一短 が ある が それぞれ が 重要 な 研究 手法 で ある <EOS>\n",
      "result(train): フランス も ある <EOS> ある <EOS> ある <EOS> ある <EOS> する <EOS> <EOS> ある <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 15/100 \n",
      "training loss: 57.52729797363281\n",
      "source(train): したがって 物 が 受け身 表現 の 主語 に なる こと は ほとんど なかっ た <EOS>\n",
      "result(train): これら ある <EOS> ある で <EOS> が に て <EOS> が ない で た <EOS> も <EOS> た た た\n",
      "epoch: 16/100 \n",
      "training loss: 60.771663665771484\n",
      "source(train): それ 以外 と なる と かなり 重要 度 が 落ちる <EOS>\n",
      "result(train): 若者 また と する する する する た さ ある <EOS> も で た が が が た が が\n",
      "epoch: 17/100 \n",
      "training loss: 50.45553970336914\n",
      "source(train): 以下 日本語 の 敬語 体系 および 敬意 表現 について 述べる <EOS>\n",
      "result(train): 自立 に は 面 で も で を が も <EOS> も が が が が が が が が\n",
      "epoch: 18/100 \n",
      "training loss: 43.89338302612305\n",
      "source(train): 若者 の 日本語 は 表記 の 面 でも 独自 性 を 持つ <EOS>\n",
      "result(train): ただし に 面 も 現在 の が で で を を する <EOS> も た た た た た た\n",
      "epoch: 19/100 \n",
      "training loss: 43.9288444519043\n",
      "source(train): これら の 形 は 今日 でも 各地 に 残っ て いる <EOS>\n",
      "result(train): ただし に 面 が 今日 に で に 分け て いる <EOS> も た た が た た が が\n",
      "epoch: 20/100 \n",
      "training loss: 38.214881896972656\n",
      "source(train): 有機物 以外 を 構成 要素 と する 生物 も 想定 さ れる <EOS>\n",
      "result(train): ただし に を する て と する <EOS> も ある <EOS> れ <EOS> も た た た た た た\n",
      "epoch: 21/100 \n",
      "training loss: 33.027042388916016\n",
      "source(train): そうした 言い方 を 習得 する こと は どの 言語 でも 容易 で ない <EOS>\n",
      "result(train): しかし 分類 を 練習 する <EOS> が ない の と で で ある <EOS> も た た が た た\n",
      "epoch: 22/100 \n",
      "training loss: 31.795289993286133\n",
      "source(train): 逆 に マルタ は 長い 間 アフリカ に 属する 島 と 受け止め られ て い た <EOS>\n",
      "result(train): ただし の 存在 は 現在 の に に 存在 て と する て て いる た <EOS> も た た\n",
      "epoch: 23/100 \n",
      "training loss: 28.54407501220703\n",
      "source(train): しかし 正確 に 数える こと は ほぼ 不可能 で ある <EOS>\n",
      "result(train): しかし に に 分け <EOS> が 今日 で で ある <EOS> も は は が の は の の の\n",
      "epoch: 24/100 \n",
      "training loss: 26.149019241333008\n",
      "source(train): 年代 により さまざま な 日本語 の 表記 が 行わ れ て いる <EOS>\n",
      "result(train): ただし 体系 に の な は 中心 が 生まれる れ て いる <EOS> も た た が は は が\n",
      "epoch: 25/100 \n",
      "training loss: 23.388418197631836\n",
      "source(train): ことに 述語 は 文 を まとめる 重要 な 役割 を 果たす <EOS>\n",
      "result(train): また は は 今日 の 練習 する な 役割 は 採る <EOS> も は が が の は が の\n",
      "epoch: 26/100 \n",
      "training loss: 20.770788192749023\n",
      "source(train): 漢字 表記 の 面 で は 地域 文字 と いう べき もの が 各地 に 存在 する <EOS>\n",
      "result(train): パリ の の 方言 で ある 地域 の と さ さ と と あっ に 存在 する <EOS> も は\n",
      "epoch: 27/100 \n",
      "training loss: 21.38357162475586\n",
      "source(train): そうした 言い方 を 習得 する こと は どの 言語 でも 容易 で ない <EOS>\n",
      "result(train): この の は 練習 する <EOS> が ない の と で で ある <EOS> も が が が が が\n",
      "epoch: 28/100 \n",
      "training loss: 16.788061141967773\n",
      "source(train): しかし 正確 に 数える こと は ほぼ 不可能 で ある <EOS>\n",
      "result(train): また の に 最も こと は ない 不可能 で ある <EOS> も が が が が が が が が\n",
      "epoch: 29/100 \n",
      "training loss: 16.18769645690918\n",
      "source(train): パリ は フランス 全土 で も 医師 の 密度 が もっとも 高い 街 の ひとつ で ある <EOS>\n",
      "result(train): パリ は 形容動詞 の で ある ある の 密度 が 生まれる 高い 街 の ひとつ で ある <EOS> も が\n",
      "epoch: 30/100 \n",
      "training loss: 15.893067359924316\n",
      "source(train): この 状態 で 虫 が つく こと も ほとんど 無い と 言う <EOS>\n",
      "result(train): この 種 の は が つく <EOS> が ある 無い と 言う <EOS> も が た が が が が\n",
      "epoch: 31/100 \n",
      "training loss: 14.711787223815918\n",
      "source(train): その ため 何 百 年 という 間 に は 話し言葉 と 書き言葉 の 差 が 生まれる <EOS>\n",
      "result(train): この ため の 百 年 という 間 に も 話し言葉 と 書き言葉 の 差 が 生まれる <EOS> も で が\n",
      "epoch: 32/100 \n",
      "training loss: 17.838897705078125\n",
      "source(train): また ほか の 高等 教育 機関 も この 地区 に 存在 する <EOS>\n",
      "result(train): パリ 社会 の 方言 教育 機関 も 多い 地区 に 存在 する <EOS> も が た た た た た\n",
      "epoch: 33/100 \n",
      "training loss: 28.916244506835938\n",
      "source(train): 他 に も 言語 に は さまざま な 分類 が ある <EOS>\n",
      "result(train): この に も 多い に 存在 さまざま な 分類 が ある <EOS> を が た が た た た た\n",
      "epoch: 34/100 \n",
      "training loss: 36.587608337402344\n",
      "source(train): パリ は フランス 全土 で も 医師 の 密度 が もっとも 高い 街 の ひとつ で ある <EOS>\n",
      "result(train): その は 目的 全土 で ある 医師 の 密度 が もっとも 高い 街 の ひとつ で ある <EOS> も が\n",
      "epoch: 35/100 \n",
      "training loss: 22.129924774169922\n",
      "source(train): この うち とりわけ 目 を 引く の は 形容詞 の 少な さ で ある <EOS>\n",
      "result(train): パリ うち とりわけ 目 を 引く の が 今日 の 少な で で ある <EOS> を が た の の\n",
      "epoch: 36/100 \n",
      "training loss: 9.178131103515625\n",
      "source(train): 年代 により さまざま な 日本語 の 表記 が 行わ れ て いる <EOS>\n",
      "result(train): この の により な 正書法 を 群 が 続い れ て いる <EOS> も は た た た た た\n",
      "epoch: 37/100 \n",
      "training loss: 18.452518463134766\n",
      "source(train): ただし 一部 の 方言 に は 今 も 残っ て いる <EOS>\n",
      "result(train): この 海水 の 方言 区画 は 今 も 残っ て いる <EOS> も は た た の た の の\n",
      "epoch: 38/100 \n",
      "training loss: 24.651037216186523\n",
      "source(train): 北 は 地中海 東 は 紅海 に 面し て いる <EOS>\n",
      "result(train): この は 地中海 東 は 紅海 に 面し て いる <EOS> も は た た た た の の た\n",
      "epoch: 39/100 \n",
      "training loss: 12.983352661132812\n",
      "source(train): この 状態 で 虫 が つく こと も ほとんど 無い と 言う <EOS>\n",
      "result(train): この うち で 虫 が つく こと も 多い 無い と 言う <EOS> を が が が が が が\n",
      "epoch: 40/100 \n",
      "training loss: 6.618736743927002\n",
      "source(train): 言語 と 方言 の 区別 について 現在 なさ れる 説明 は 二つ で ある <EOS>\n",
      "result(train): この に 方言 の 区別 について 現在 なさ れる 説明 は 二つ で ある <EOS> を が が が が\n",
      "epoch: 41/100 \n",
      "training loss: 16.245880126953125\n",
      "source(train): ここ で は その 中 で 国 で ある 可能 性 が ある もの を 列挙 する <EOS>\n",
      "result(train): この の は 以下 中 で 国 で ある 可能 性 が ある もの を 列挙 する <EOS> を が\n",
      "epoch: 42/100 \n",
      "training loss: 16.10665512084961\n",
      "source(train): いわば 漢文 を 日本語 に 直訳 し ながら 読む もの で あっ た <EOS>\n",
      "result(train): パリ 漢文 を 日本語 に 直訳 し ながら 読む もの で あっ た <EOS> も は た た た た\n",
      "epoch: 43/100 \n",
      "training loss: 6.281676292419434\n",
      "source(train): この よう な 語 の うち 日本語 として 定着 し た 語 も 多い <EOS>\n",
      "result(train): この 種 に の の うち 日本語 として 定着 し た 語 も 多い <EOS> も が た た た\n",
      "epoch: 44/100 \n",
      "training loss: 6.945587635040283\n",
      "source(train): また 社会 心理 学 や 小 集団 実験 と 関連 する 研究 も 多い <EOS>\n",
      "result(train): その ほか 心理 学 や 小 集団 実験 と 関連 する 研究 も 多い <EOS> も が た た が\n",
      "epoch: 45/100 \n",
      "training loss: 13.212850570678711\n",
      "source(train): この 種 の 文 を 読み 慣れ た 人 で なけれ ば 分かり にくい <EOS>\n",
      "result(train): この うち の 批判 を 読み 慣れ た 人 で なけれ ば 分かり にくい <EOS> も が た た た\n",
      "epoch: 46/100 \n",
      "training loss: 10.889521598815918\n",
      "source(train): そして その 中心 に 社会 学 を 位置づけ た の で ある <EOS>\n",
      "result(train): ただし その 中心 に 社会 学 を 位置づけ た の で ある <EOS> を が た の の の の\n",
      "epoch: 47/100 \n",
      "training loss: 5.155546188354492\n",
      "source(train): パリ 市 は 県庁 所在地 と さ れ て い た <EOS>\n",
      "result(train): この 市内 は 県庁 所在地 と さ れ て い た <EOS> も た た た た た た た\n",
      "epoch: 48/100 \n",
      "training loss: 4.321680545806885\n",
      "source(train): それ 以外 と なる と かなり 重要 度 が 落ちる <EOS>\n",
      "result(train): したがって ゆえ と なる と かなり 重要 度 が 落ちる <EOS> を が た た た た た た た\n",
      "epoch: 49/100 \n",
      "training loss: 9.120016098022461\n",
      "source(train): 文 は 目的 や 場面 など に 応じ て さまざま な 異なっ た 様式 を 採る <EOS>\n",
      "result(train): この を 目的 や 場面 など に 応じ て さまざま な 異なっ た 様式 を 採る <EOS> を が た\n",
      "epoch: 50/100 \n",
      "training loss: 10.881014823913574\n",
      "source(train): この 種 の 批判 は 古典 文学 の 中 に も 見 られる <EOS>\n",
      "result(train): この うち の 批判 は 古典 文学 の 中 に も 見 られる <EOS> も は は た た た\n",
      "epoch: 51/100 \n",
      "training loss: 7.112700939178467\n",
      "source(train): 人間 は 歴史 社会 を 創造 する とともに 歴史 社会 の なか を 生きる 存在 で ある <EOS>\n",
      "result(train): この は 歴史 社会 を 創造 する とともに 歴史 社会 の なか を 生きる 存在 で ある <EOS> を が\n",
      "epoch: 52/100 \n",
      "training loss: 2.906690835952759\n",
      "source(train): パリ 大学 は 特に 神学 の 研究 で 著名 で あっ た <EOS>\n",
      "result(train): この 大学 は 特に 神学 の 研究 で 著名 で あっ た <EOS> も は た た た た た\n",
      "epoch: 53/100 \n",
      "training loss: 3.3589389324188232\n",
      "source(train): また ほか の 高等 教育 機関 も この 地区 に 存在 する <EOS>\n",
      "result(train): この ほか の 高等 教育 機関 も この 地区 に 存在 する <EOS> も が た た た た た\n",
      "epoch: 54/100 \n",
      "training loss: 6.209039688110352\n",
      "source(train): この 文字 を 練習 する ため の 本 も 出版 さ れ た <EOS>\n",
      "result(train): この 種 を 練習 する ため の 本 も 出版 さ れ た <EOS> も た た た た た\n",
      "epoch: 55/100 \n",
      "training loss: 9.19857406616211\n",
      "source(train): 年代 により さまざま な 日本語 の 表記 が 行わ れ て いる <EOS>\n",
      "result(train): この により さまざま な 日本語 の 表記 が 行わ れ て いる <EOS> も は は は た た の\n",
      "epoch: 56/100 \n",
      "training loss: 9.246910095214844\n",
      "source(train): また 社会 心理 学 や 小 集団 実験 と 関連 する 研究 も 多い <EOS>\n",
      "result(train): この ほか 心理 学 や 小 集団 実験 と 関連 する 研究 も 多い <EOS> も は た た た\n",
      "epoch: 57/100 \n",
      "training loss: 7.342218399047852\n",
      "source(train): 逆 に マルタ は 長い 間 アフリカ に 属する 島 と 受け止め られ て い た <EOS>\n",
      "result(train): この に マルタ は 長い 間 アフリカ に 属する 島 と 受け止め られ て い た <EOS> も は た\n",
      "epoch: 58/100 \n",
      "training loss: 4.407296180725098\n",
      "source(train): また 街 の 予算 は 国 の 同意 を 得る 必要 が あっ た <EOS>\n",
      "result(train): この 街 の 予算 は 国 の 同意 を 得る 必要 が あっ た <EOS> も た た た た\n",
      "epoch: 59/100 \n",
      "training loss: 2.3344616889953613\n",
      "source(train): 名詞 や 形容動詞 語幹 など も 独立 語 として 用い られる <EOS>\n",
      "result(train): この や 形容動詞 語幹 など も 独立 語 として 用い られる <EOS> は は は は た た た た\n",
      "epoch: 60/100 \n",
      "training loss: 1.6371877193450928\n",
      "source(train): 主 な 格 助詞 と その 典型 的 な 機能 は 次 の 通り で ある <EOS>\n",
      "result(train): この な 格 助詞 と その 典型 的 な 機能 は 次 の 通り で ある <EOS> を で た\n",
      "epoch: 61/100 \n",
      "training loss: 2.1931819915771484\n",
      "source(train): パリ は フランス 全土 で も 医師 の 密度 が もっとも 高い 街 の ひとつ で ある <EOS>\n",
      "result(train): この は フランス 全土 で も 医師 の 密度 が もっとも 高い 街 の ひとつ で ある <EOS> も で\n",
      "epoch: 62/100 \n",
      "training loss: 3.8455770015716553\n",
      "source(train): この 他 教育 学部 に 設置 さ れ て いる 大学 も ある <EOS>\n",
      "result(train): その うち 教育 学部 に 設置 さ れ て いる 大学 も ある <EOS> も は た は の の\n",
      "epoch: 63/100 \n",
      "training loss: 7.53527307510376\n",
      "source(train): また 街 の 予算 は 国 の 同意 を 得る 必要 が あっ た <EOS>\n",
      "result(train): この 街 の 予算 は 国 の 同意 を 得る 必要 が あっ た <EOS> も た た た た\n",
      "epoch: 64/100 \n",
      "training loss: 15.442882537841797\n",
      "source(train): また ほか の 高等 教育 機関 も この 地区 に 存在 する <EOS>\n",
      "result(train): この 街 の 高等 教育 機関 も この 地区 に 存在 する <EOS> も が た た た た た\n",
      "epoch: 65/100 \n",
      "training loss: 31.102214813232422\n",
      "source(train): この 状態 で 虫 が つく こと も ほとんど 無い と 言う <EOS>\n",
      "result(train): この 種 で 虫 が つく こと も ほとんど 無い と 言う <EOS> を を た た た た た\n",
      "epoch: 66/100 \n",
      "training loss: 60.4906120300293\n",
      "source(train): また 社会 心理 学 や 小 集団 実験 と 関連 する 研究 も 多い <EOS>\n",
      "result(train): パリ 社会 心理 学 や 小 集団 実験 と 関連 する 研究 も 多い <EOS> も は た た た\n",
      "epoch: 67/100 \n",
      "training loss: 82.40736389160156\n",
      "source(train): 変体 仮名 は 現在 でも 料理 屋 の 名 など に 使わ れる こと が ある <EOS>\n",
      "result(train): この 仮名 は 現在 でも 料理 屋 の 名 など に 使わ れる こと が ある <EOS> も が た\n",
      "epoch: 68/100 \n",
      "training loss: 74.41314697265625\n",
      "source(train): 第 二 の もの は まず どの 方言 も それぞれ 言語 だ と する <EOS>\n",
      "result(train): この 二 の もの は まず どの 方言 も それぞれ 言語 だ と する <EOS> も が た た た\n",
      "epoch: 69/100 \n",
      "training loss: 54.45269775390625\n",
      "source(train): 建築 材料 として は 日干し レンガ が 主流 で あり 石材 も 多用 さ れ た <EOS>\n",
      "result(train): この 材料 として は 日干し レンガ が 主流 で あり 石材 も 多用 さ れ た <EOS> も た た\n",
      "epoch: 70/100 \n",
      "training loss: 34.96940612792969\n",
      "source(train): パリ 大学 は 特に 神学 の 研究 で 著名 で あっ た <EOS>\n",
      "result(train): 文 市 は 特に 神学 の 研究 で 著名 で あっ た <EOS> を は た た た た た\n",
      "epoch: 71/100 \n",
      "training loss: 24.540870666503906\n",
      "source(train): また 社会 心理 学 や 小 集団 実験 と 関連 する 研究 も 多い <EOS>\n",
      "result(train): この 社会 心理 学 や 小 集団 実験 と 関連 する 研究 も 多い <EOS> も は た た た\n",
      "epoch: 72/100 \n",
      "training loss: 38.273799896240234\n",
      "source(train): その ため 何 百 年 という 間 に は 話し言葉 と 書き言葉 の 差 が 生まれる <EOS>\n",
      "result(train): この ため 何 百 年 という 間 に は 話し言葉 と 書き言葉 の 差 が 生まれる <EOS> も が は\n",
      "epoch: 73/100 \n",
      "training loss: 35.13886260986328\n",
      "source(train): また 社会 心理 学 や 小 集団 実験 と 関連 する 研究 も 多い <EOS>\n",
      "result(train): この 社会 心理 学 や 小 集団 実験 と 関連 する 研究 も 多い <EOS> も は た た た\n",
      "epoch: 74/100 \n",
      "training loss: 8.492506980895996\n",
      "source(train): その ため 何 百 年 という 間 に は 話し言葉 と 書き言葉 の 差 が 生まれる <EOS>\n",
      "result(train): ただし ため 何 百 年 という 間 に は 話し言葉 と 書き言葉 の 差 が 生まれる <EOS> も が は\n",
      "epoch: 75/100 \n",
      "training loss: 22.99599266052246\n",
      "source(train): 重要 な 点 は 三つ の 群 で 大きく 異なる が 共通 する こと も 多い <EOS>\n",
      "result(train): 多く な 点 は 三つ の 群 で 大きく 異なる が 共通 する こと も 多い <EOS> も が た\n",
      "epoch: 76/100 \n",
      "training loss: 23.82356834411621\n",
      "source(train): 多く の 場合 において 規則 的 な 対応 が 見 られる <EOS>\n",
      "result(train): この の 場合 において 規則 的 な 対応 が 見 られる <EOS> は は は は は は た た\n",
      "epoch: 77/100 \n",
      "training loss: 5.848620891571045\n",
      "source(train): 建築 材料 として は 日干し レンガ が 主流 で あり 石材 も 多用 さ れ た <EOS>\n",
      "result(train): パリ 材料 として は 日干し レンガ が 主流 で あり 石材 も 多用 さ れ た <EOS> も た た\n",
      "epoch: 78/100 \n",
      "training loss: 16.77985954284668\n",
      "source(train): それ 以外 と なる と かなり 重要 度 が 落ちる <EOS>\n",
      "result(train): この 以外 と なる と かなり 重要 度 が 落ちる <EOS> を が た た た た た た た\n",
      "epoch: 79/100 \n",
      "training loss: 13.564911842346191\n",
      "source(train): どれ も 一長一短 が ある が それぞれ が 重要 な 研究 手法 で ある <EOS>\n",
      "result(train): この も 一長一短 が ある が それぞれ が 重要 な 研究 手法 で ある <EOS> を が は は は\n",
      "epoch: 80/100 \n",
      "training loss: 5.348544597625732\n",
      "source(train): 他 に も 言語 に は さまざま な 分類 が ある <EOS>\n",
      "result(train): この に も 言語 に は さまざま な 分類 が ある <EOS> を が た た た た た た\n",
      "epoch: 81/100 \n",
      "training loss: 14.035250663757324\n",
      "source(train): その後 今日 に 至る まで 何 度 か 移民 の 波 が 続い て いる <EOS>\n",
      "result(train): この 今日 に 至る まで 何 度 か 移民 の 波 が 続い て いる <EOS> も た た た\n",
      "epoch: 82/100 \n",
      "training loss: 10.443212509155273\n",
      "source(train): この うち とりわけ 目 を 引く の は 形容詞 の 少な さ で ある <EOS>\n",
      "result(train): ただし 種 最も 目 を 引く の は 形容詞 の 少な さ で ある <EOS> を は は た た\n",
      "epoch: 83/100 \n",
      "training loss: 7.365673065185547\n",
      "source(train): 重要 な 点 は 三つ の 群 で 大きく 異なる が 共通 する こと も 多い <EOS>\n",
      "result(train): この な 点 は 三つ の 群 で 大きく 異なる が 共通 する こと も 多い <EOS> も が た\n",
      "epoch: 84/100 \n",
      "training loss: 8.506556510925293\n",
      "source(train): 漢字 表記 の 面 で は 地域 文字 と いう べき もの が 各地 に 存在 する <EOS>\n",
      "result(train): この 表記 の 面 で は 地域 文字 と いう べき もの が 各地 に 存在 する <EOS> も は\n",
      "epoch: 85/100 \n",
      "training loss: 6.473849773406982\n",
      "source(train): この ギャル 文字 を 練習 する ため の 本 も 現れ た <EOS>\n",
      "result(train): この ギャル 文字 を 練習 する ため の 本 も 現れ た <EOS> も た た た た た た\n",
      "epoch: 86/100 \n",
      "training loss: 7.074406147003174\n",
      "source(train): また 社会 心理 学 や 小 集団 実験 と 関連 する 研究 も 多い <EOS>\n",
      "result(train): この 社会 心理 学 や 小 集団 実験 と 関連 する 研究 も 多い <EOS> も は た た た\n",
      "epoch: 87/100 \n",
      "training loss: 4.979294776916504\n",
      "source(train): パリ 大学 は 特に 神学 の 研究 で 著名 で あっ た <EOS>\n",
      "result(train): この 大学 は 特に 神学 の 研究 で 著名 で あっ た <EOS> も は た た た た た\n",
      "epoch: 88/100 \n",
      "training loss: 4.577490329742432\n",
      "source(train): そして その 中心 に 社会 学 を 位置づけ た の で ある <EOS>\n",
      "result(train): この その 中心 に 社会 学 を 位置づけ た の で ある <EOS> を は た た た た た\n",
      "epoch: 89/100 \n",
      "training loss: 6.431400299072266\n",
      "source(train): 外国 人 による 日本語 研究 も 中世 末期 から 近世 前期 にかけて 多く 行わ れ た <EOS>\n",
      "result(train): この 人 による 日本語 研究 も 中世 末期 から 近世 前期 にかけて 多く 行わ れ た <EOS> は た た\n",
      "epoch: 90/100 \n",
      "training loss: 4.2318525314331055\n",
      "source(train): 数 多く の 病院 が パリ に 設置 さ れ て いる <EOS>\n",
      "result(train): この 多く の 病院 が パリ に 設置 さ れ て いる <EOS> も は た は は た た\n",
      "epoch: 91/100 \n",
      "training loss: 4.001425743103027\n",
      "source(train): ただし 厳密 な 正書法 は なく 表記 の ゆれ は 広く 許容 さ れ て いる <EOS>\n",
      "result(train): この 厳密 な 正書法 は なく 表記 の ゆれ は 広く 許容 さ れ て いる <EOS> も は は\n",
      "epoch: 92/100 \n",
      "training loss: 3.4990854263305664\n",
      "source(train): どれ も 一長一短 が ある が それぞれ が 重要 な 研究 手法 で ある <EOS>\n",
      "result(train): この も 一長一短 が ある が それぞれ が 重要 な 研究 手法 で ある <EOS> を が は は た\n",
      "epoch: 93/100 \n",
      "training loss: 3.2956149578094482\n",
      "source(train): 変体 仮名 は 現在 でも 料理 屋 の 名 など に 使わ れる こと が ある <EOS>\n",
      "result(train): この 仮名 は 現在 でも 料理 屋 の 名 など に 使わ れる こと が ある <EOS> も が た\n",
      "epoch: 94/100 \n",
      "training loss: 4.161630630493164\n",
      "source(train): もともと 少ない 形容詞 を 補う 主要 な 形式 は 形容動詞 で ある <EOS>\n",
      "result(train): その 少ない 形容詞 を 補う 主要 な 形式 は 形容動詞 で ある <EOS> を は た た た た た\n",
      "epoch: 95/100 \n",
      "training loss: 2.9756460189819336\n",
      "source(train): 漢語 は 音読み で 読ま れる こと から 字音 語 と 呼ば れる 場合 も ある <EOS>\n",
      "result(train): この は 音読み で 読ま れる こと から 字音 語 と 呼ば れる 場合 も ある <EOS> を が た\n",
      "epoch: 96/100 \n",
      "training loss: 2.361521005630493\n",
      "source(train): また ほか の 高等 教育 機関 も この 地区 に 存在 する <EOS>\n",
      "result(train): この ほか の 高等 教育 機関 も この 地区 に 存在 する <EOS> も は た た た た た\n",
      "epoch: 97/100 \n",
      "training loss: 2.2291312217712402\n",
      "source(train): パリ は 年間 外国 人 観光 客数 が 世界一 の 観光 都市 で ある <EOS>\n",
      "result(train): この は 年間 外国 人 観光 客数 が 世界一 の 観光 都市 で ある <EOS> を は は は の\n",
      "epoch: 98/100 \n",
      "training loss: 2.003568649291992\n",
      "source(train): また ほか の 高等 教育 機関 も この 地区 に 存在 する <EOS>\n",
      "result(train): この ほか の 高等 教育 機関 も この 地区 に 存在 する <EOS> も が た た た た た\n",
      "epoch: 99/100 \n",
      "training loss: 3.116729736328125\n",
      "source(train): 品詞 分類 で は 独立 語 として のみ 用い られる 品詞 は 感動 詞 と さ れる <EOS>\n",
      "result(train): この 分類 で は 独立 語 として のみ 用い られる 品詞 は 感動 詞 と さ れる <EOS> を が\n",
      "epoch: 100/100 \n",
      "training loss: 5.1369500160217285\n",
      "source(train): 同音 語 を アクセント によって 区別 できる 場合 も 少なく ない <EOS>\n",
      "result(train): 文 語 を アクセント によって 区別 できる 場合 も 少なく ない <EOS> も で が の の の の の\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "学習を始める\n",
    "'''\n",
    "print(\"start train...\")\n",
    "auto_encoder=AutoEncoder(maxlen=MAX_LEN)\n",
    "auto_encoder.to(device)\n",
    "optimizer = optim.Adam(auto_encoder.parameters(), lr=ae_lr)\n",
    "history_train=[]\n",
    "history_eval=[]\n",
    "\n",
    "for epoch in range(AE_EPOCHS):\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.864137172698975,\n",
       " 397.9302978515625,\n",
       " 140.99566650390625,\n",
       " 490.1822814941406,\n",
       " 274.2007751464844,\n",
       " 167.2931671142578,\n",
       " 195.38124084472656,\n",
       " 191.14199829101562,\n",
       " 123.7173080444336,\n",
       " 112.40652465820312,\n",
       " 104.27486419677734,\n",
       " 79.95535278320312,\n",
       " 82.34085083007812,\n",
       " 72.95103454589844,\n",
       " 57.52729797363281,\n",
       " 60.771663665771484,\n",
       " 50.45553970336914,\n",
       " 43.89338302612305,\n",
       " 43.9288444519043,\n",
       " 38.214881896972656,\n",
       " 33.027042388916016,\n",
       " 31.795289993286133,\n",
       " 28.54407501220703,\n",
       " 26.149019241333008,\n",
       " 23.388418197631836,\n",
       " 20.770788192749023,\n",
       " 21.38357162475586,\n",
       " 16.788061141967773,\n",
       " 16.18769645690918,\n",
       " 15.893067359924316,\n",
       " 14.711787223815918,\n",
       " 17.838897705078125,\n",
       " 28.916244506835938,\n",
       " 36.587608337402344,\n",
       " 22.129924774169922,\n",
       " 9.178131103515625,\n",
       " 18.452518463134766,\n",
       " 24.651037216186523,\n",
       " 12.983352661132812,\n",
       " 6.618736743927002,\n",
       " 16.245880126953125,\n",
       " 16.10665512084961,\n",
       " 6.281676292419434,\n",
       " 6.945587635040283,\n",
       " 13.212850570678711,\n",
       " 10.889521598815918,\n",
       " 5.155546188354492,\n",
       " 4.321680545806885,\n",
       " 9.120016098022461,\n",
       " 10.881014823913574,\n",
       " 7.112700939178467,\n",
       " 2.906690835952759,\n",
       " 3.3589389324188232,\n",
       " 6.209039688110352,\n",
       " 9.19857406616211,\n",
       " 9.246910095214844,\n",
       " 7.342218399047852,\n",
       " 4.407296180725098,\n",
       " 2.3344616889953613,\n",
       " 1.6371877193450928,\n",
       " 2.1931819915771484,\n",
       " 3.8455770015716553,\n",
       " 7.53527307510376,\n",
       " 15.442882537841797,\n",
       " 31.102214813232422,\n",
       " 60.4906120300293,\n",
       " 82.40736389160156,\n",
       " 74.41314697265625,\n",
       " 54.45269775390625,\n",
       " 34.96940612792969,\n",
       " 24.540870666503906,\n",
       " 38.273799896240234,\n",
       " 35.13886260986328,\n",
       " 8.492506980895996,\n",
       " 22.99599266052246,\n",
       " 23.82356834411621,\n",
       " 5.848620891571045,\n",
       " 16.77985954284668,\n",
       " 13.564911842346191,\n",
       " 5.348544597625732,\n",
       " 14.035250663757324,\n",
       " 10.443212509155273,\n",
       " 7.365673065185547,\n",
       " 8.506556510925293,\n",
       " 6.473849773406982,\n",
       " 7.074406147003174,\n",
       " 4.979294776916504,\n",
       " 4.577490329742432,\n",
       " 6.431400299072266,\n",
       " 4.2318525314331055,\n",
       " 4.001425743103027,\n",
       " 3.4990854263305664,\n",
       " 3.2956149578094482,\n",
       " 4.161630630493164,\n",
       " 2.9756460189819336,\n",
       " 2.361521005630493,\n",
       " 2.2291312217712402,\n",
       " 2.003568649291992,\n",
       " 3.116729736328125,\n",
       " 5.1369500160217285]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttext確認用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nihongo_id=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「方言」の似ている単語\n",
      "方法:tensor(21.5570, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "ひとつ:tensor(21.7216, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "うち:tensor(21.8818, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "独自:tensor(21.9748, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "県庁:tensor(22.0037, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "品詞:tensor(22.0942, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "ゆれ:tensor(22.1198, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "日干し:tensor(22.1313, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "にくい:tensor(22.2012, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "生活:tensor(22.2619, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "nihongo_id+=1\n",
    "norm_list=[]\n",
    "word=TEXT.vocab.itos[nihongo_id]\n",
    "nihongo_vector=share_conv.weight[nihongo_id]\n",
    "for i, val in enumerate(share_conv.weight):\n",
    "    diff=nihongo_vector-val\n",
    "    norm=torch.norm(diff)\n",
    "    norm_list.append((norm,i))\n",
    "norm_list.sort()\n",
    "print(\"「\"+word+\"」の似ている単語\")\n",
    "for i in range(1,11):\n",
    "    val=norm_list[i][0]\n",
    "    id_=norm_list[i][1]\n",
    "    print(TEXT.vocab.itos[id_]+\":\"+str(val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36-yasuda",
   "language": "python",
   "name": "py36-yasuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
