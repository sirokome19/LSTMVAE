{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data, datasets\n",
    "from torchtext.vocab import FastText\n",
    "\n",
    "import os\n",
    "import math\n",
    "import MeCab\n",
    "import shutil\n",
    "\n",
    "device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 1\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "mecab = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "# BLEU\n",
    "\n",
    "from Util.selfbleu import CalcSelfBLEU\n",
    "from Util.bleu import calc_all_bleu\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchwordemb\n",
    "w2v, vec = torchwordemb.load_word2vec_text('./data/vector/model.vec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPER PARAMETER\n",
    "BATCH_SIZE=100\n",
    "EMBED_SIZE=300\n",
    "LSTM_HIDDEN_SIZE=300\n",
    "AE_EPOCHS=100\n",
    "REPORT_INTERVAL=1\n",
    "ae_lr=1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL NAME\n",
    "data_source=\"wiki\"\n",
    "feature=\"only_ja_small\"\n",
    "feature=\"only_ja_sample\"\n",
    "# data_source=\"orphans\"\n",
    "# feature=\"all\"\n",
    "\n",
    "log_dir='./log/{}/'.format(\"_\".join([\"train\",data_source,feature]))\n",
    "if os.path.isdir(log_dir):\n",
    "    shutil.rmtree(log_dir)\n",
    "os.mkdir(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_source==\"wiki\" or data_source==\"orphans\":\n",
    "    MAX_LEN=20 #paddingをバッチにかかわらず固定長にする\n",
    "    TEXT = data.Field(sequential=True, lower=True, include_lengths=True, init_token=\"<SOS>\",eos_token=\"<EOS>\", fix_length=MAX_LEN, batch_first=True)\n",
    "    train, val, test = data.TabularDataset.splits(\n",
    "        path='./data/foruse', train=\"_\".join([\"train\",data_source,feature])+'.tsv',\n",
    "        validation=\"_\".join([\"eval\",data_source,feature])+'.tsv',test=\"_\".join([\"test\",data_source,feature])+'.tsv', \n",
    "        format='tsv',\n",
    "        fields=[('Text', TEXT),])\n",
    "#     TEXT.build_vocab(train, vectors=FastText(language=\"ja\"))\n",
    "    TEXT.build_vocab(train)\n",
    "    TEXT.vocab.set_vectors(stoi=w2v, vectors=vec, dim=300)\n",
    "    train_iter, val_iter, test_iter = data.Iterator.splits((train, val, test), batch_sizes=(BATCH_SIZE, BATCH_SIZE, BATCH_SIZE),sort = False, device=torch.device(device))\n",
    "    vocab_size=TEXT.vocab.vectors.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [ 0.1511, -0.0320, -0.3042,  ...,  0.0911, -0.3855, -0.0442],\n",
       "        [-0.1153, -0.2428, -0.2427,  ..., -0.1985,  0.2873, -0.2767],\n",
       "        [ 0.2523, -0.1367,  0.1136,  ...,  0.1502,  0.0156,  0.0910]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "share_conv=nn.Embedding(vocab_size, EMBED_SIZE).to(device)\n",
    "share_conv.weight.data.copy_(TEXT.vocab.vectors)\n",
    "# share_conv.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,embed_size,n_hid):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size = embed_size,\n",
    "                            hidden_size = n_hid,\n",
    "                            batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        input: \n",
    "            x = (batch, max_len, embed_dim)\n",
    "        output:\n",
    "            x = (batch, hidden_size)\n",
    "        \"\"\"\n",
    "        _, (h,c)=self.lstm(x) # h=(max_len, batch, n_hid)\n",
    "        return (h,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vae_classifier_2layer(nn.Module):\n",
    "    def __init__(self,n_hid):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(n_hid*2, n_hid)\n",
    "        self.fcmean = nn.Linear(n_hid, n_hid)\n",
    "        self.fcvar = nn.Linear(n_hid, n_hid)\n",
    "        self.ReLU=nn.ReLU()\n",
    "        self.lnorm=nn.LayerNorm(n_hid)\n",
    "\n",
    "    def forward(self, hidden):# h,c = (batch, hidden_size)\n",
    "        h, c = hidden\n",
    "        tmp = self.ReLU(self.lnorm(self.fc(torch.cat([h,c],dim=-1))))\n",
    "        mean = self.fcmean(tmp)\n",
    "        log_sigma_sq = self.fcvar(tmp)\n",
    "        return mean, log_sigma_sq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,embed_size,n_hid,max_len,vocab_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size = embed_size,\n",
    "                            hidden_size = n_hid,\n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(n_hid, vocab_size)\n",
    "        self.max_len=max_len\n",
    "        self.embedding=share_conv\n",
    "        \n",
    "    def forward(self, hidden, x, teacher):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            x_emb=(batch, hidden_size, embedding_size)\n",
    "        output:\n",
    "            logits = (batch, max_len, vocab_size)\n",
    "            sentence = (batch, max_len) : 中身はindex\n",
    "        \"\"\"\n",
    "        logits = torch.tensor([]).to(device)\n",
    "        sentence = torch.tensor([],dtype=torch.long).to(device)\n",
    "        \n",
    "        for i in range(self.max_len):\n",
    "            if teacher or i==0:\n",
    "                tmp = torch.unsqueeze(x[:,i,:],1) # tmp = (batch, 1, embed_dim)\n",
    "            else:\n",
    "                # word = (batch, 1, 1)\n",
    "                tmp = self.embedding(word) # tmp = (batch, 1, embed_dim)\n",
    "                x_emb = F.normalize(tmp)\n",
    "            x_input = tmp # x_input = (batch, 1, (embed_size + n_hid))\n",
    "            out, hidden = self.lstm(x_input, hidden)\n",
    "                # out = (batch, 1, n_hid)\n",
    "                # hidden = ((batch, 1, n_hid),(batch, 1, n_hid))\n",
    "            logit = self.fc(out) # logit = (batch, 1, vocab_size)\n",
    "            word = torch.argmax(logit, dim=2) # word = (batch, 1)\n",
    "\n",
    "            sentence = torch.cat([sentence, word],1)\n",
    "            logits = torch.cat([logits,logit],1)   \n",
    "        return logits, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self,maxlen):\n",
    "        super().__init__()\n",
    "        self.n_hid=LSTM_HIDDEN_SIZE\n",
    "        self.maxlen=maxlen\n",
    "        self.encoder=Encoder(embed_size=EMBED_SIZE,n_hid=self.n_hid)\n",
    "        self.vae_classifer=vae_classifier_2layer(n_hid=self.n_hid)\n",
    "        self.decoder=Decoder(embed_size=EMBED_SIZE,n_hid=self.n_hid,max_len=MAX_LEN,vocab_size=vocab_size)\n",
    "        self.embedding=share_conv\n",
    "        self.fc1 = nn.Linear(self.n_hid, self.n_hid)\n",
    "        self.fc2 = nn.Linear(self.n_hid, self.n_hid)\n",
    "        \n",
    "    def forward(self, x): # x=(batch, max_len)\n",
    "        x_emb=self.embedding(x)  # x_emb = (batch, maxlen, embed_dim)\n",
    "        x_emb = F.normalize(x_emb)\n",
    "        \n",
    "        hidden = self.encoder(x_emb) # h,c = (1, batch, hidden_size)\n",
    "        mean, log_sigma_sq=self.vae_classifer(hidden)\n",
    "        eps = torch.empty(len(x), self.n_hid).normal_(mean=0,std=1).to(device) # N(0, 1)\n",
    "        h = mean + eps * torch.sqrt(torch.exp(log_sigma_sq)) # H_dec = (1, batch, n_gan)\n",
    "        h,c=self.fc1(h), self.fc2(h)\n",
    "        logits, sentence = self.decoder((h,c), x_emb, teacher=True)\n",
    "        \n",
    "        # loss\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=1)\n",
    "        loss = 0\n",
    "        for i in range(self.maxlen-1):\n",
    "            # <SOS>を除くためindexをずらす\n",
    "            loss += criterion(torch.squeeze(logits[:,i,:]), torch.squeeze(x[:,i+1]))\n",
    "        loss/=(self.maxlen-1)\n",
    "        \n",
    "        # KL loss\n",
    "        # 標準正規分布と(μ,σ^2)正規分布を仮定しているので以下の計算式になる\n",
    "        # nn.klDivLossを使うと仮定が甘い\n",
    "        # kl_loss = Σ0.5Σ(μ^2+exp(ln(σ^2))-ln(σ^2)-1)を使う\n",
    "        kl_loss = torch.sum(0.5 * torch.sum((mean**2 + torch.exp(log_sigma_sq) - log_sigma_sq - 1),dim=1))\n",
    "        loss += epoch/AE_EPOCHS*kl_loss\n",
    "        \n",
    "        return loss, sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_to_sent(sents,j):\n",
    "    word_list=[]\n",
    "    for i in sents[j]:\n",
    "        if i==TEXT.vocab.stoi[\"<EOS>\"]:\n",
    "            break\n",
    "        if i!=TEXT.vocab.stoi[\"<PAD>\"]:\n",
    "            word_list.append(TEXT.vocab.itos[int(i)])\n",
    "    return word_list\n",
    "\n",
    "def write_out(url, origin_sents, syn_sents):\n",
    "    with open(url, \"a\") as f:\n",
    "        for j in range(len(syn_sents)):\n",
    "            f.write(\"input : \"+\" \".join(change_to_sent(origin_sents,j))+\"\\n\")\n",
    "            f.write(\"output: \"+\" \".join(change_to_sent(syn_sents,j))+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    auto_encoder.train()\n",
    "    epoch_loss = 0\n",
    "    count=0\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "        (x, x_l) = batch.Text\n",
    "            # xには文章のID表現が、x_lにはxの単語数が入る\n",
    "            # x=(batch, max_len)\n",
    "        if len(x)!=BATCH_SIZE:break\n",
    "        optimizer.zero_grad()\n",
    "        loss, syn_sents=auto_encoder(x)\n",
    "        with torch.autograd.detect_anomaly():\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss+=loss.item()\n",
    "        count+=1\n",
    "        \n",
    "    sample_x=x[0][1:]\n",
    "    source_sentence=' '.join([TEXT.vocab.itos[int(i)] for i in sample_x if i != 1])\n",
    "    gen_sentence=' '.join([TEXT.vocab.itos[int(i)] for i in syn_sents[0] if i != 1])\n",
    "\n",
    "    write_out(log_dir+\"{:03}.txt\".format(epoch), x[:,1:], syn_sents)\n",
    "\n",
    "    history_train.append(epoch_loss/count)\n",
    "    if (epoch+1) % REPORT_INTERVAL==0:\n",
    "        print(\"epoch: \"+str(epoch+1)+'/'+str(AE_EPOCHS)+' ')\n",
    "        print(\"training loss: \"+str(history_train[epoch]))\n",
    "#         print(\"kl_loss: \"+str(kl_loss))\n",
    "        print(\"source(train): \"+str(source_sentence))\n",
    "        print(\"result(train): \"+str(gen_sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start train...\n",
      "epoch: 1/100 \n",
      "training loss: 5.874156951904297\n",
      "source(train): 品詞 分類 で は 常に 接続 語 と なる 品詞 を 接続詞 と する <EOS>\n",
      "result(train): 形式 直訳 受け止め 金 多く 全土 少なく 心理 心理 品詞 独立 心理 場 心理 名 名 名 構成 構成 構成\n",
      "epoch: 2/100 \n",
      "training loss: 415.3277587890625\n",
      "source(train): ただし 海水 中 に 生育 する もの は 確認 さ れ て い ない <EOS>\n",
      "result(train): の の の <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 3/100 \n",
      "training loss: 129.82948303222656\n",
      "source(train): 語 によって は 特定 の 尊敬 語 が 対応 する もの も ある <EOS>\n",
      "result(train): <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 4/100 \n",
      "training loss: 466.90423583984375\n",
      "source(train): 自立 語 で 活用 の ない もの の うち 主語 に なる もの を 名詞 と する <EOS>\n",
      "result(train): 性格 記号 区画 <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 5/100 \n",
      "training loss: 221.52378845214844\n",
      "source(train): 第 二 の もの は まず どの 方言 も それぞれ 言語 だ と する <EOS>\n",
      "result(train): <EOS> ある ある ある ある ある ある ある の ある ある の ある ある ある ある ある ある ある ある\n",
      "epoch: 6/100 \n",
      "training loss: 121.29983520507812\n",
      "source(train): パリ は フランス 全土 で も 医師 の 密度 が もっとも 高い 街 の ひとつ で ある <EOS>\n",
      "result(train): の の の の の の の の の の の の の の の の の の の の\n",
      "epoch: 7/100 \n",
      "training loss: 173.38397216796875\n",
      "source(train): 漢語 は 音読み で 読ま れる こと から 字音 語 と 呼ば れる 場合 も ある <EOS>\n",
      "result(train): 有機物 有機物 の の の の の <EOS> <EOS> の の の <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 8/100 \n",
      "training loss: 95.88011169433594\n",
      "source(train): 特に 移民 や 植民 地 など で フランス 色 が 強い 都市 に 多い <EOS>\n",
      "result(train): ただし の の の の <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 9/100 \n",
      "training loss: 108.9791488647461\n",
      "source(train): 有機物 以外 を 構成 要素 と する 生物 も 想定 さ れる <EOS>\n",
      "result(train): <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 10/100 \n",
      "training loss: 84.43109130859375\n",
      "source(train): 同音 語 を アクセント によって 区別 できる 場合 も 少なく ない <EOS>\n",
      "result(train): <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 11/100 \n",
      "training loss: 68.83515167236328\n",
      "source(train): そうした 言い方 を 習得 する こと は どの 言語 でも 容易 で ない <EOS>\n",
      "result(train): この は は は は <EOS> <EOS> <EOS> <EOS> は <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 12/100 \n",
      "training loss: 80.46643829345703\n",
      "source(train): この 種 の 批判 は 古典 文学 の 中 に も 見 られる <EOS>\n",
      "result(train): この は は は は <EOS> は は <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 13/100 \n",
      "training loss: 54.722957611083984\n",
      "source(train): 季節 による 雨量 の 変化 も 少なく ほぼ 一定 し て いる <EOS>\n",
      "result(train): この は は の <EOS> の <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 14/100 \n",
      "training loss: 57.655941009521484\n",
      "source(train): 移民 の 居住 区域 は それぞれ 出身 地 ごと に 異なっ て いる <EOS>\n",
      "result(train): この の <EOS> の の <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 15/100 \n",
      "training loss: 53.43016052246094\n",
      "source(train): 日本 で 生まれ 育っ た ほとんど の 人 は 日本語 を 母語 と する <EOS>\n",
      "result(train): 漢語 の <EOS> の の <EOS> <EOS> <EOS> <EOS> <EOS> の <EOS> の <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 16/100 \n",
      "training loss: 48.82244110107422\n",
      "source(train): また 社会 心理 学 や 小 集団 実験 と 関連 する 研究 も 多い <EOS>\n",
      "result(train): その <EOS> の の の の の の の <EOS> で <EOS> で <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 17/100 \n",
      "training loss: 43.48982238769531\n",
      "source(train): これら 化学 反応 が おこる 場 を 提供 し て いる の が 水 で ある <EOS>\n",
      "result(train): この の の の <EOS> で の <EOS> が <EOS> <EOS> <EOS> <EOS> <EOS> で ある <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 18/100 \n",
      "training loss: 48.951683044433594\n",
      "source(train): また 社会 心理 学 や 小 集団 実験 と 関連 する 研究 も 多い <EOS>\n",
      "result(train): られる <EOS> の の の の の の の <EOS> <EOS> <EOS> で <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 19/100 \n",
      "training loss: 38.055259704589844\n",
      "source(train): この 種 の 批判 は 古典 文学 の 中 に も 見 られる <EOS>\n",
      "result(train): ただし <EOS> の ある の ある の の ある で ある <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 20/100 \n",
      "training loss: 34.240177154541016\n",
      "source(train): そうした 言い方 を 習得 する こと は どの 言語 でも 容易 で ない <EOS>\n",
      "result(train): そうした の は 練習 の <EOS> も ある で と に で ある <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 21/100 \n",
      "training loss: 29.239397048950195\n",
      "source(train): 文 を 成り立た せる 基本 的 な 成分 で ある <EOS>\n",
      "result(train): この は は は は 的 に ある で ある <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 22/100 \n",
      "training loss: 28.929771423339844\n",
      "source(train): ことに 述語 は 文 を まとめる 重要 な 役割 を 果たす <EOS>\n",
      "result(train): <EOS> で は 活用 を ある <EOS> で ある で ある <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 23/100 \n",
      "training loss: 26.561185836791992\n",
      "source(train): 人間 は 歴史 社会 を 創造 する とともに 歴史 社会 の なか を 生きる 存在 で ある <EOS>\n",
      "result(train): この の 方言 の の 練習 の <EOS> 存在 の の 中心 で ある <EOS> で ある <EOS> <EOS> <EOS>\n",
      "epoch: 24/100 \n",
      "training loss: 33.641727447509766\n",
      "source(train): パリ は フランス 全土 で も 医師 の 密度 が もっとも 高い 街 の ひとつ で ある <EOS>\n",
      "result(train): この は 日本語 の の ある ある の ある が ある さ <EOS> で ある で ある <EOS> <EOS> <EOS>\n",
      "epoch: 25/100 \n",
      "training loss: 37.82535171508789\n",
      "source(train): どれ も 一長一短 が ある が それぞれ が 重要 な 研究 手法 で ある <EOS>\n",
      "result(train): この の 方言 が ある <EOS> ある が ある で ある で で ある <EOS> が <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 26/100 \n",
      "training loss: 35.61355972290039\n",
      "source(train): 文 は 目的 や 場面 など に 応じ て さまざま な 異なっ た 様式 を 採る <EOS>\n",
      "result(train): この は 日本語 の 植民 など に 存在 て いる で ある て <EOS> で 列挙 <EOS> も <EOS> <EOS>\n",
      "epoch: 27/100 \n",
      "training loss: 21.998106002807617\n",
      "source(train): 多く の 場合 において 規則 的 な 対応 が 見 られる <EOS>\n",
      "result(train): この の 方言 において は は に 対応 が ある られる <EOS> も <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 28/100 \n",
      "training loss: 16.04523468017578\n",
      "source(train): 建築 材料 として は 日干し レンガ が 主流 で あり 石材 も 多用 さ れ た <EOS>\n",
      "result(train): この の や は 今 レンガ が ある で ある ある で ある さ れ て <EOS> も <EOS> <EOS>\n",
      "epoch: 29/100 \n",
      "training loss: 23.37550926208496\n",
      "source(train): 名詞 や 形容動詞 語幹 など も 独立 語 として 用い られる <EOS>\n",
      "result(train): この は 形容動詞 は など は ある 語 と する られる <EOS> も <EOS> <EOS> <EOS> <EOS> 存在 存在 存在\n",
      "epoch: 30/100 \n",
      "training loss: 30.52703857421875\n",
      "source(train): そうした 言い方 を 習得 する こと は どの 言語 でも 容易 で ない <EOS>\n",
      "result(train): この 以外 を 練習 する こと も ある ば と 容易 で ある <EOS> も ある が が が が\n",
      "epoch: 31/100 \n",
      "training loss: 21.788990020751953\n",
      "source(train): その ため 何 百 年 という 間 に は 話し言葉 と 書き言葉 の 差 が 生まれる <EOS>\n",
      "result(train): この ため 領域 百 年 という こと に 設置 話し言葉 の する の 差 が 生まれる <EOS> も が が\n",
      "epoch: 32/100 \n",
      "training loss: 14.515962600708008\n",
      "source(train): 名詞 や 形容動詞 語幹 など も 独立 語 として 用い られる <EOS>\n",
      "result(train): この 分類 形容動詞 語幹 など は 活用 語 と 用い られる <EOS> も <EOS> が が が が が が\n",
      "epoch: 33/100 \n",
      "training loss: 10.647201538085938\n",
      "source(train): その 温室 効果 によって 地表 の 温度 も 高かっ た <EOS>\n",
      "result(train): この ため 効果 を 地表 の 中心 が ある た <EOS> も <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 34/100 \n",
      "training loss: 14.60926628112793\n",
      "source(train): パリ 大学 は 特に 神学 の 研究 で 著名 で あっ た <EOS>\n",
      "result(train): この 市 は 現在 神学 の 区別 で ある で ある た <EOS> も が が が が が が\n",
      "epoch: 35/100 \n",
      "training loss: 21.724184036254883\n",
      "source(train): 地球 以外 に 生命 が 発見 さ れ た 事例 は 記録 さ れ て い ない <EOS>\n",
      "result(train): パリ 以外 に は が 発見 さ れ て <EOS> も 記録 さ れ て いる ない <EOS> も が\n",
      "epoch: 36/100 \n",
      "training loss: 22.09259605407715\n",
      "source(train): 地球 以外 に 生命 が 発見 さ れ た 事例 は 記録 さ れ て い ない <EOS>\n",
      "result(train): この 以外 に は が 発見 さ れ て <EOS> も 記録 さ れ て いる ない <EOS> も が\n",
      "epoch: 37/100 \n",
      "training loss: 19.076313018798828\n",
      "source(train): 語 によって は 特定 の 尊敬 語 が 対応 する もの も ある <EOS>\n",
      "result(train): この の は 今 の よう 語 と 対応 する <EOS> も ある <EOS> も が が が が が\n",
      "epoch: 38/100 \n",
      "training loss: 11.281330108642578\n",
      "source(train): 季節 による 雨量 の 変化 も 少なく ほぼ 一定 し て いる <EOS>\n",
      "result(train): この による 雨量 の 変化 も 少なく ほぼ 一定 し て いる <EOS> も が が が が が が\n",
      "epoch: 39/100 \n",
      "training loss: 7.087717533111572\n",
      "source(train): 日本語 が 時 と共に 変化 する こと は しばしば 批判 の 対象 と なる <EOS>\n",
      "result(train): 品詞 の 時 と共に 変化 する こと が しばしば 交わさ は 対象 と なる <EOS> も が が が が\n",
      "epoch: 40/100 \n",
      "training loss: 7.2252631187438965\n",
      "source(train): 自立 語 で 活用 の ない もの の うち 主語 に なる もの を 名詞 と する <EOS>\n",
      "result(train): しかし 語 は は の ない もの と ある 主語 に なる もの と 名詞 と する <EOS> も が\n",
      "epoch: 41/100 \n",
      "training loss: 9.07265853881836\n",
      "source(train): 一つ の 方言 区画 の 内部 も 変化 に 富ん で いる <EOS>\n",
      "result(train): この の 方言 区画 の 内部 も 変化 に 富ん で ある <EOS> も は が が が が が\n",
      "epoch: 42/100 \n",
      "training loss: 14.8944091796875\n",
      "source(train): 文 を 成り立た せる 基本 的 な 成分 で ある <EOS>\n",
      "result(train): また を 成り立た せる 基本 的 な 成分 で ある <EOS> で が が が が が が が が\n",
      "epoch: 43/100 \n",
      "training loss: 22.559659957885742\n",
      "source(train): 名詞 や 形容動詞 語幹 など も 独立 語 として 用い られる <EOS>\n",
      "result(train): この や 形容動詞 語幹 など も 独立 語 として のみ られる <EOS> も が が が が が が が\n",
      "epoch: 44/100 \n",
      "training loss: 33.38884735107422\n",
      "source(train): 第 二 の もの は まず どの 方言 も それぞれ 言語 だ と する <EOS>\n",
      "result(train): この 二 の 日本語 も まず どの 方言 の それぞれ 言語 だ と する <EOS> も が が が が\n",
      "epoch: 45/100 \n",
      "training loss: 43.274658203125\n",
      "source(train): この 説明 方法 は 現在 の 学校 教育 の 国語 でも 取り入れ られ て いる <EOS>\n",
      "result(train): この 種 方法 は 現在 の 学校 教育 の 国語 でも 取り入れ られ て いる <EOS> も が が が\n",
      "epoch: 46/100 \n",
      "training loss: 40.12584686279297\n",
      "source(train): 一方 形容動詞 は 今日 に 至る まで 高い 造語 力 を 保っ て いる <EOS>\n",
      "result(train): その 形容動詞 は 今日 に 至る まで 高い 造語 力 を 保っ て いる <EOS> も が が が が\n",
      "epoch: 47/100 \n",
      "training loss: 20.90677261352539\n",
      "source(train): 今日 でも この 体系 は 基本 的 に 変わっ て い ない <EOS>\n",
      "result(train): 日本語 でも この 体系 は 基本 的 に 変わっ て いる た <EOS> も が が が が が が\n",
      "epoch: 48/100 \n",
      "training loss: 4.47127103805542\n",
      "source(train): 今日 でも この 体系 は 基本 的 に 変わっ て い ない <EOS>\n",
      "result(train): ただし でも この 体系 は 基本 的 に 変わっ て いる ない <EOS> も が が が が が が\n",
      "epoch: 49/100 \n",
      "training loss: 8.286324501037598\n",
      "source(train): パリ は フランス 全土 で も 医師 の 密度 が もっとも 高い 街 の ひとつ で ある <EOS>\n",
      "result(train): <EOS> に フランス 全土 で も 医師 の 密度 が もっとも 高い 街 の ひとつ で ある <EOS> が が\n",
      "epoch: 50/100 \n",
      "training loss: 22.09955406188965\n",
      "source(train): その後 今日 に 至る まで 何 度 か 移民 の 波 が 続い て いる <EOS>\n",
      "result(train): この 今日 に 至る まで 何 度 か 移民 の 波 が 続い て いる <EOS> も が が が\n",
      "epoch: 51/100 \n",
      "training loss: 26.11794090270996\n",
      "source(train): ただし 海水 中 に 生育 する もの は 確認 さ れ て い ない <EOS>\n",
      "result(train): ただし ナ 中 に 生育 する もの は 確認 さ れ て い ない <EOS> も が が が が\n",
      "epoch: 52/100 \n",
      "training loss: 15.014068603515625\n",
      "source(train): これ によって 初めて 日本語 を 自由 に 記す こと が 可能 に なっ た <EOS>\n",
      "result(train): この によって 初めて 日本語 を 自由 に 記す こと が 可能 に なっ た <EOS> も が が が が\n",
      "epoch: 53/100 \n",
      "training loss: 3.5910391807556152\n",
      "source(train): 語族 は さらに 語 派 語 群 そして 言語 と 細分 化 さ れ て いく <EOS>\n",
      "result(train): この は さらに 語 派 語 群 そして 言語 と 細分 化 さ れ て いく <EOS> も が ある\n",
      "epoch: 54/100 \n",
      "training loss: 4.69686222076416\n",
      "source(train): 日本語 で は 限り なく 長い 複合語 を 作る こと が 可能 で ある <EOS>\n",
      "result(train): この が は 限り なく 長い 複合語 を 作る こと が 可能 で ある <EOS> が が が が が\n",
      "epoch: 55/100 \n",
      "training loss: 13.533467292785645\n",
      "source(train): 対照 的 に 最も 小さな 国家 は バチカン 市 国 で ある <EOS>\n",
      "result(train): ただし 的 に 最も 小さな 国家 は バチカン 市 国 で ある <EOS> の が が が が が が\n",
      "epoch: 56/100 \n",
      "training loss: 17.406593322753906\n",
      "source(train): パリ 市 は 県庁 所在地 と さ れ て い た <EOS>\n",
      "result(train): この は は 県庁 所在地 と さ れ て い た <EOS> も が ある が が が が が\n",
      "epoch: 57/100 \n",
      "training loss: 12.622542381286621\n",
      "source(train): その後 今日 に 至る まで 何 度 か 移民 の 波 が 続い て いる <EOS>\n",
      "result(train): この 今日 に 至る まで 何 度 か 移民 の 波 が 続い て いる <EOS> が が が が\n",
      "epoch: 58/100 \n",
      "training loss: 4.63998556137085\n",
      "source(train): その こと を ガイア と も 呼ぶ もの も ある <EOS>\n",
      "result(train): この ため を ガイア と も 呼ぶ もの も ある <EOS> も が が が が が が が が\n",
      "epoch: 59/100 \n",
      "training loss: 2.0372984409332275\n",
      "source(train): どれ も 一長一短 が ある が それぞれ が 重要 な 研究 手法 で ある <EOS>\n",
      "result(train): この も 一長一短 が ある が それぞれ が 重要 な 研究 手法 で ある <EOS> が が が が が\n",
      "epoch: 60/100 \n",
      "training loss: 5.678095817565918\n",
      "source(train): また ほか の 高等 教育 機関 も この 地区 に 存在 する <EOS>\n",
      "result(train): その 社会 の 高等 教育 機関 も この 地区 に 存在 する <EOS> が が ある が が が が\n",
      "epoch: 61/100 \n",
      "training loss: 10.755158424377441\n",
      "source(train): ただし 海水 中 に 生育 する もの は 確認 さ れ て い ない <EOS>\n",
      "result(train): は 一部 中 に 生育 する もの は 確認 さ れ て い ない <EOS> も が ある が が\n",
      "epoch: 62/100 \n",
      "training loss: 13.369311332702637\n",
      "source(train): そして その 中心 に 社会 学 を 位置づけ た の で ある <EOS>\n",
      "result(train): この その 中心 に 社会 学 を 位置づけ た の で ある <EOS> が が ある が が が が\n",
      "epoch: 63/100 \n",
      "training loss: 11.955082893371582\n",
      "source(train): その 温室 効果 によって 地表 の 温度 も 高かっ た <EOS>\n",
      "result(train): この ため 効果 によって 地表 の 温度 も 高かっ た <EOS> も が ある が が が が が が\n",
      "epoch: 64/100 \n",
      "training loss: 8.396211624145508\n",
      "source(train): 他 に も 言語 に は さまざま な 分類 が ある <EOS>\n",
      "result(train): この に も 言語 に は さまざま な 分類 が ある <EOS> が が が が が が が が\n",
      "epoch: 65/100 \n",
      "training loss: 4.976497650146484\n",
      "source(train): 文 は 目的 や 場面 など に 応じ て さまざま な 異なっ た 様式 を 採る <EOS>\n",
      "result(train): 名詞 を 目的 や 場面 など に 応じ て さまざま な 異なっ た 様式 を 採る <EOS> が が ある\n",
      "epoch: 66/100 \n",
      "training loss: 4.343523979187012\n",
      "source(train): そして その 中心 に 社会 学 を 位置づけ た の で ある <EOS>\n",
      "result(train): この その 中心 に 社会 学 を 位置づけ た の で ある <EOS> が が ある が が が が\n",
      "epoch: 67/100 \n",
      "training loss: 8.901354789733887\n",
      "source(train): また 社会 心理 学 や 小 集団 実験 と 関連 する 研究 も 多い <EOS>\n",
      "result(train): この ほか 心理 学 や 小 集団 実験 と 関連 する 研究 も 多い <EOS> が が が が が\n",
      "epoch: 68/100 \n",
      "training loss: 21.945096969604492\n",
      "source(train): 名詞 や 形容動詞 語幹 など も 独立 語 として 用い られる <EOS>\n",
      "result(train): ただし や 形容動詞 語幹 など も 独立 語 として 用い られる <EOS> も ある ある さ さ さ さ さ\n",
      "epoch: 69/100 \n",
      "training loss: 44.44749069213867\n",
      "source(train): 今日 でも この 体系 は 基本 的 に 変わっ て い ない <EOS>\n",
      "result(train): この でも この 体系 は 基本 的 に 変わっ て い ない <EOS> も が が が が が が\n",
      "epoch: 70/100 \n",
      "training loss: 60.25423812866211\n",
      "source(train): パリ 大学 は 特に 神学 の 研究 で 著名 で あっ た <EOS>\n",
      "result(train): この 大学 は 特に 神学 の 研究 で 著名 で あっ た <EOS> が が が が が が が\n",
      "epoch: 71/100 \n",
      "training loss: 37.40374755859375\n",
      "source(train): フランス 最大 の 都市 で あり 同国 の 政治 経済 文化 など の 中心 で ある <EOS>\n",
      "result(train): この 最大 の 都市 で あり 同国 の 政治 経済 文化 など の 中心 で ある <EOS> を ある ある\n",
      "epoch: 72/100 \n",
      "training loss: 10.109840393066406\n",
      "source(train): 一つ の 方言 区画 の 内部 も 変化 に 富ん で いる <EOS>\n",
      "result(train): この の 方言 区画 の 内部 も 変化 に 富ん で いる <EOS> も が が が が が が\n",
      "epoch: 73/100 \n",
      "training loss: 25.314001083374023\n",
      "source(train): 建築 材料 として は 日干し レンガ が 主流 で あり 石材 も 多用 さ れ た <EOS>\n",
      "result(train): この 材料 として は 日干し レンガ が 主流 で あり 石材 も 多用 さ れ た <EOS> が が が\n",
      "epoch: 74/100 \n",
      "training loss: 46.54729461669922\n",
      "source(train): 変体 仮名 は 現在 でも 料理 屋 の 名 など に 使わ れる こと が ある <EOS>\n",
      "result(train): 品詞 仮名 は 現在 でも 料理 屋 の 名 など に 使わ れる こと が ある <EOS> が ある ある\n",
      "epoch: 75/100 \n",
      "training loss: 31.909292221069336\n",
      "source(train): 表記 体系 は ほか の 諸 言語 と 比べ て 複雑 で ある <EOS>\n",
      "result(train): ただし 体系 は ほか の 諸 言語 と 比べ て 複雑 で ある <EOS> が ある ある ある が が\n",
      "epoch: 76/100 \n",
      "training loss: 30.30787467956543\n",
      "source(train): その ため 領域 は 観念 的 な もの で ある <EOS>\n",
      "result(train): この ため 領域 は 観念 的 な もの で ある <EOS> <EOS> ある ある ある ある ある ある 重要 重要\n",
      "epoch: 77/100 \n",
      "training loss: 55.95883560180664\n",
      "source(train): 地理 学 誕生 の 地 は 古代 ギリシア で ある <EOS>\n",
      "result(train): この 学 誕生 の 地 は 古代 ギリシア で ある <EOS> の ある ある ある ある ある ある ある ある\n",
      "epoch: 78/100 \n",
      "training loss: 45.173728942871094\n",
      "source(train): ただし 厳密 な 正書法 は なく 表記 の ゆれ は 広く 許容 さ れ て いる <EOS>\n",
      "result(train): この 厳密 な 正書法 は なく 表記 の ゆれ は 広く 許容 さ れ て いる <EOS> も ある ある\n",
      "epoch: 79/100 \n",
      "training loss: 29.038715362548828\n",
      "source(train): 一つ の 方言 区画 の 内部 も 変化 に 富ん で いる <EOS>\n",
      "result(train): この の 方言 区画 の 内部 も 変化 に 富ん で いる <EOS> も が が が が が が\n",
      "epoch: 80/100 \n",
      "training loss: 31.932191848754883\n",
      "source(train): 今日 でも この 体系 は 基本 的 に 変わっ て い ない <EOS>\n",
      "result(train): この でも この 体系 は 基本 的 に 変わっ て い ない <EOS> も ある が が が が が\n",
      "epoch: 81/100 \n",
      "training loss: 14.778406143188477\n",
      "source(train): パリ 大学 は 特に 神学 の 研究 で 著名 で あっ た <EOS>\n",
      "result(train): この 大学 は 特に 神学 の 研究 で 著名 で あっ た <EOS> が が ある が が ある ある\n",
      "epoch: 82/100 \n",
      "training loss: 4.239598751068115\n",
      "source(train): 古代 エジプト は 次 の 時代 に 区分 さ れ て いる <EOS>\n",
      "result(train): 品詞 エジプト は 次 の 時代 に 区分 さ れ て いる <EOS> も ある ある ある が が が\n",
      "epoch: 83/100 \n",
      "training loss: 15.255255699157715\n",
      "source(train): 他 に も 言語 に は さまざま な 分類 が ある <EOS>\n",
      "result(train): 特に に も 言語 に は さまざま な 分類 が ある <EOS> が が が が ある ある 語 語\n",
      "epoch: 84/100 \n",
      "training loss: 15.590850830078125\n",
      "source(train): 第 二 の もの は まず どの 方言 も それぞれ 言語 だ と する <EOS>\n",
      "result(train): この 二 の もの は まず どの 方言 も それぞれ 言語 だ と する <EOS> も が ある ある ある\n",
      "epoch: 85/100 \n",
      "training loss: 15.527873039245605\n",
      "source(train): 他 に も 言語 に は さまざま な 分類 が ある <EOS>\n",
      "result(train): ただし に も 言語 に は さまざま な 分類 が ある <EOS> が が が ある ある 語 語 語\n",
      "epoch: 86/100 \n",
      "training loss: 21.02607536315918\n",
      "source(train): 数 多く の 病院 が パリ に 設置 さ れ て いる <EOS>\n",
      "result(train): この 多く の 病院 が パリ に 設置 さ れ て いる <EOS> も が ある が が が が\n",
      "epoch: 87/100 \n",
      "training loss: 13.424264907836914\n",
      "source(train): 古代 エジプト は 次 の 時代 に 区分 さ れ て いる <EOS>\n",
      "result(train): この エジプト は 次 の 時代 に 区分 さ れ て いる <EOS> も ある ある ある が が が\n",
      "epoch: 88/100 \n",
      "training loss: 6.501131057739258\n",
      "source(train): その こと を ガイア と も 呼ぶ もの も ある <EOS>\n",
      "result(train): この こと を ガイア と も 呼ぶ もの も ある <EOS> も ある ある ある ある が が が <EOS>\n",
      "epoch: 89/100 \n",
      "training loss: 8.026741981506348\n",
      "source(train): この 川 の 中州 で ある シテ 島 を 中心 に 発達 し た <EOS>\n",
      "result(train): その 種 の 中州 で ある シテ 島 を 中心 に 発達 し た <EOS> が が ある が が\n",
      "epoch: 90/100 \n",
      "training loss: 6.137632369995117\n",
      "source(train): これら 化学 反応 が おこる 場 を 提供 し て いる の が 水 で ある <EOS>\n",
      "result(train): この 化学 反応 が おこる 場 を 提供 し て いる の が 水 で ある <EOS> が ある ある\n",
      "epoch: 91/100 \n",
      "training loss: 5.715387344360352\n",
      "source(train): 音素 記号 で は 以下 の よう に 記さ れる <EOS>\n",
      "result(train): この 記号 で は 以下 の よう に 記さ れる <EOS> が ある ある ある ある ある ある が が\n",
      "epoch: 92/100 \n",
      "training loss: 10.1294584274292\n",
      "source(train): この 種 の 批判 は 古典 文学 の 中 に も 見 られる <EOS>\n",
      "result(train): この 種 の 批判 は 古典 文学 の 中 に も 見 られる <EOS> も ある ある ある ある ある\n",
      "epoch: 93/100 \n",
      "training loss: 17.900609970092773\n",
      "source(train): 今日 でも この 体系 は 基本 的 に 変わっ て い ない <EOS>\n",
      "result(train): パリ でも この 体系 は 基本 的 に 変わっ て い ない <EOS> も ある ある が が が 名詞\n",
      "epoch: 94/100 \n",
      "training loss: 41.25621795654297\n",
      "source(train): しかし 正確 に 数える こと は ほぼ 不可能 で ある <EOS>\n",
      "result(train): この 正確 に 数える こと は ほぼ 不可能 で ある <EOS> <EOS> ある ある ある ある ある が が <EOS>\n",
      "epoch: 95/100 \n",
      "training loss: 73.13386535644531\n",
      "source(train): 文 を 成り立た せる 基本 的 な 成分 で ある <EOS>\n",
      "result(train): この を 成り立た せる 基本 的 な 成分 で ある <EOS> <EOS> ある ある ある ある ある ある 名詞 名詞\n",
      "epoch: 96/100 \n",
      "training loss: 81.2337875366211\n",
      "source(train): 地球 以外 に 生命 が 発見 さ れ た 事例 は 記録 さ れ て い ない <EOS>\n",
      "result(train): この 以外 に 生命 が 発見 さ れ た 事例 は 記録 さ れ て い ない <EOS> <EOS> ある\n",
      "epoch: 97/100 \n",
      "training loss: 47.633949279785156\n",
      "source(train): それ ゆえ に 日本語 は 開 音節 言語 の 性格 が 強い という こと が できる <EOS>\n",
      "result(train): この ゆえ に 日本語 は 開 音節 言語 の 性格 が 強い という こと が できる <EOS> が が が\n",
      "epoch: 98/100 \n",
      "training loss: 11.929142951965332\n",
      "source(train): その ため 何 百 年 という 間 に は 話し言葉 と 書き言葉 の 差 が 生まれる <EOS>\n",
      "result(train): 品詞 ため 何 百 年 という 間 に は 話し言葉 と 書き言葉 の 差 が 生まれる <EOS> も が ある\n",
      "epoch: 99/100 \n",
      "training loss: 18.714614868164062\n",
      "source(train): 変体 仮名 は 現在 でも 料理 屋 の 名 など に 使わ れる こと が ある <EOS>\n",
      "result(train): この 仮名 は 現在 でも 料理 屋 の 名 など に 使わ れる こと が ある <EOS> が ある ある\n",
      "epoch: 100/100 \n",
      "training loss: 39.20659637451172\n",
      "source(train): パリ は フランス 全土 で も 医師 の 密度 が もっとも 高い 街 の ひとつ で ある <EOS>\n",
      "result(train): この は フランス 全土 で も 医師 の 密度 が もっとも 高い 街 の ひとつ で ある <EOS> が ある\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "学習を始める\n",
    "'''\n",
    "print(\"start train...\")\n",
    "auto_encoder=AutoEncoder(maxlen=MAX_LEN)\n",
    "auto_encoder.to(device)\n",
    "optimizer = optim.Adam(auto_encoder.parameters(), lr=ae_lr)\n",
    "history_train=[]\n",
    "history_eval=[]\n",
    "\n",
    "for epoch in range(AE_EPOCHS):\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [i for i in range(len(history_train))]\n",
    "plt.plot(x, history_train, label=\"train\")\n",
    "# plt.plot(x, history_eval, label=\"eval\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttext確認用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nihongo_id=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「その」の似ている単語\n",
      "この:tensor(0.7487, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "の:tensor(0.6892, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "が:tensor(0.6426, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "た:tensor(0.6121, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "それ:tensor(0.6105, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "これ:tensor(0.6037, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "も:tensor(0.5796, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "を:tensor(0.5789, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "しかし:tensor(0.5774, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "に:tensor(0.5679, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "nihongo_id+=1\n",
    "norm_list=[]\n",
    "word=TEXT.vocab.itos[nihongo_id]\n",
    "nihongo_vector=share_conv.weight[nihongo_id]\n",
    "for i, val in enumerate(share_conv.weight):\n",
    "    cos=torch.nn.CosineSimilarity(dim=0)\n",
    "    diff=cos(nihongo_vector,val)\n",
    "    norm=torch.norm(diff)\n",
    "    norm_list.append((norm,i))\n",
    "norm_list.sort(reverse=True)\n",
    "print(\"「\"+word+\"」の似ている単語\")\n",
    "for i in range(1,11):\n",
    "    val=norm_list[i][0]\n",
    "    id_=norm_list[i][1]\n",
    "    print(TEXT.vocab.itos[id_]+\":\"+str(val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36-yasuda",
   "language": "python",
   "name": "py36-yasuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
