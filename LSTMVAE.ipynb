{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data, datasets\n",
    "from torchtext.vocab import FastText\n",
    "\n",
    "import os\n",
    "import math\n",
    "import MeCab\n",
    "\n",
    "device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 1\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "mecab = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "# BLEU\n",
    "\n",
    "from Util.selfbleu import CalcSelfBLEU\n",
    "from Util.bleu import calc_all_bleu\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPER PARAMETER\n",
    "BATCH_SIZE=100\n",
    "EMBED_SIZE=300\n",
    "LSTM_HIDDEN_SIZE=300\n",
    "AE_EPOCHS=100\n",
    "REPORT_INTERVAL=5\n",
    "ae_lr=1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL NAME\n",
    "data_source=\"wiki\"\n",
    "# feature=\"only_ja_small\"\n",
    "feature=\"only_ja_sample\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_source==\"wiki\":\n",
    "    MAX_LEN=20 #paddingをバッチにかかわらず固定長にする\n",
    "    TEXT = data.Field(sequential=True, lower=True, include_lengths=True, init_token=\"<SOS>\",eos_token=\"<EOS>\", fix_length=MAX_LEN, batch_first=True)\n",
    "    train, val, test = data.TabularDataset.splits(\n",
    "        path='./data/foruse', train=\"_\".join([\"train\",data_source,feature])+'.tsv',\n",
    "        validation=\"_\".join([\"eval\",data_source,feature])+'.tsv',test=\"_\".join([\"test\",data_source,feature])+'.tsv', \n",
    "        format='tsv',\n",
    "        fields=[('Text', TEXT),])\n",
    "    TEXT.build_vocab(train, vectors=FastText(language=\"ja\"))\n",
    "    train_iter, val_iter, test_iter = data.Iterator.splits((train, val, test), batch_sizes=(BATCH_SIZE, BATCH_SIZE, BATCH_SIZE),sort = False, device=torch.device(device))\n",
    "    vocab_size=TEXT.vocab.vectors.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_conv=nn.Embedding(vocab_size, EMBED_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,embed_size,n_hid):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size = embed_size,\n",
    "                            hidden_size = n_hid,\n",
    "                            batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        input: \n",
    "            x = (batch, max_len, embed_dim)\n",
    "        output:\n",
    "            x = (batch, hidden_size)\n",
    "        \"\"\"\n",
    "        _, (h,c)=self.lstm(x) # h=(max_len, batch, n_hid)\n",
    "        return (h,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,embed_size,n_hid,max_len,vocab_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size = embed_size,\n",
    "                            hidden_size = n_hid,\n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(n_hid, vocab_size)\n",
    "        self.lnorm=nn.LayerNorm(n_hid)\n",
    "        self.max_len=max_len\n",
    "        self.embedding=share_conv\n",
    "        \n",
    "    def forward(self, hidden, x, teacher):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            x_emb=(batch, hidden_size, embedding_size)\n",
    "        output:\n",
    "            logits = (batch, max_len, vocab_size)\n",
    "            sentence = (batch, max_len) : 中身はindex\n",
    "        \"\"\"\n",
    "        logits = torch.tensor([]).to(device)\n",
    "        sentence = torch.tensor([],dtype=torch.long).to(device)\n",
    "        \n",
    "        for i in range(self.max_len):\n",
    "            if teacher or i==0:\n",
    "                tmp = torch.unsqueeze(x[:,i,:],1) # tmp = (batch, 1, embed_dim)\n",
    "            else:\n",
    "                # word = (batch, 1, 1)\n",
    "                tmp = self.embedding(word) # tmp = (batch, 1, embed_dim)\n",
    "            x_input = tmp # x_input = (batch, 1, (embed_size + n_hid))\n",
    "            out, hidden = self.lstm(x_input, hidden)\n",
    "                # out = (batch, 1, n_hid)\n",
    "                # hidden = ((batch, 1, n_hid),(batch, 1, n_hid))\n",
    "            logit = self.fc(out) # logit = (batch, 1, vocab_size)\n",
    "            word = torch.argmax(logit, dim=2) # word = (batch, 1)\n",
    "\n",
    "            sentence = torch.cat([sentence, word],1)\n",
    "            logits = torch.cat([logits,logit],1)   \n",
    "        return logits, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self,maxlen):\n",
    "        super().__init__()\n",
    "        self.maxlen=maxlen\n",
    "        self.encoder=Encoder(embed_size=EMBED_SIZE,n_hid=LSTM_HIDDEN_SIZE)\n",
    "        self.decoder=Decoder(embed_size=EMBED_SIZE,n_hid=LSTM_HIDDEN_SIZE,max_len=MAX_LEN,vocab_size=vocab_size)\n",
    "        self.embedding=share_conv\n",
    "        \n",
    "    def forward(self, x): # x=(batch, max_len)\n",
    "        x_emb=self.embedding(x)  # x_emb = (batch, maxlen, embed_dim)\n",
    "        \n",
    "        hidden = self.encoder(x_emb) # h,c = (batch, hidden_size)\n",
    "        logits, sentence = self.decoder(hidden, x_emb, teacher=True)\n",
    "        \n",
    "        # loss\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=1)\n",
    "        loss = 0\n",
    "        for i in range(self.maxlen-1):\n",
    "            # <SOS>を除くためindexをずらす\n",
    "            loss += criterion(torch.squeeze(logits[:,i,:]), torch.squeeze(x[:,i+1]))\n",
    "        loss/=(self.maxlen-1)\n",
    "        \"\"\"\n",
    "        # KL loss\n",
    "        # 標準正規分布と(μ,σ^2)正規分布を仮定しているので以下の計算式になる\n",
    "        # nn.klDivLossを使うと仮定が甘い\n",
    "        # kl_loss = Σ0.5Σ(μ^2+exp(ln(σ^2))-ln(σ^2)-1)を使う\n",
    "        kl_loss = torch.sum(0.5 * torch.sum((H_mean**2 + torch.exp(H_log_sigma_sq) - H_log_sigma_sq - 1),dim=1))\n",
    "        loss += epoch/ae_epoch_number*kl_loss\n",
    "        \"\"\"\n",
    "        \n",
    "        return loss, sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    auto_encoder.train()\n",
    "    epoch_loss = 0\n",
    "    count=0\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "        (x, x_l) = batch.Text\n",
    "            # xには文章のID表現が、x_lにはxの単語数が入る\n",
    "            # x=(batch, max_len)\n",
    "        if len(x)!=BATCH_SIZE:break\n",
    "        optimizer.zero_grad()\n",
    "        loss, syn_sents=auto_encoder(x)\n",
    "        with torch.autograd.detect_anomaly():\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss+=loss.item()\n",
    "        count+=1\n",
    "    sample_x=x[0][1:]\n",
    "    source_sentence=' '.join([TEXT.vocab.itos[int(i)] for i in sample_x if i != 1])\n",
    "    gen_sentence=' '.join([TEXT.vocab.itos[int(i)] for i in syn_sents[0] if i != 1])\n",
    "    history_train.append(epoch_loss/count)\n",
    "    if (epoch+1) % REPORT_INTERVAL==0:\n",
    "        print(\"epoch: \"+str(epoch+1)+'/'+str(AE_EPOCHS)+' ')\n",
    "        print(\"training loss: \"+str(history_train[epoch]))\n",
    "#         print(\"kl_loss: \"+str(kl_loss))\n",
    "        print(\"source(test): \"+str(source_sentence))\n",
    "        print(\"result(test): \"+str(gen_sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "学習を始める\n",
    "'''\n",
    "print(\"start train...\")\n",
    "auto_encoder=AutoEncoder(maxlen=MAX_LEN)\n",
    "auto_encoder.to(device)\n",
    "optimizer = optim.Adam(auto_encoder.parameters(), lr=ae_lr)\n",
    "history_train=[]\n",
    "history_eval=[]\n",
    "\n",
    "for epoch in range(AE_EPOCHS):\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36-yasuda",
   "language": "python",
   "name": "py36-yasuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
