{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data, datasets\n",
    "from torchtext.vocab import FastText\n",
    "\n",
    "import os\n",
    "import math\n",
    "import MeCab\n",
    "import shutil\n",
    "\n",
    "device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 1\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "mecab = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "# BLEU\n",
    "\n",
    "from Util.selfbleu import CalcSelfBLEU\n",
    "from Util.bleu import calc_all_bleu\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchwordemb\n",
    "w2v, vec = torchwordemb.load_word2vec_text('./data/vector/model.vec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPER PARAMETER\n",
    "BATCH_SIZE=100\n",
    "EMBED_SIZE=300\n",
    "LSTM_HIDDEN_SIZE=300\n",
    "KL_START_EPOCH=15\n",
    "AE_EPOCHS=100\n",
    "REPORT_INTERVAL=1\n",
    "ae_lr=1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL NAME\n",
    "data_source=\"wiki\"\n",
    "feature=\"only_ja_small\"\n",
    "feature=\"only_ja_sample\"\n",
    "# data_source=\"orphans\"\n",
    "# feature=\"all\"\n",
    "\n",
    "log_dir='./log/{}/'.format(\"_\".join([\"train\",data_source,feature]))\n",
    "if os.path.isdir(log_dir):\n",
    "    shutil.rmtree(log_dir)\n",
    "os.mkdir(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_source==\"wiki\" or data_source==\"orphans\":\n",
    "    MAX_LEN=20 #paddingをバッチにかかわらず固定長にする\n",
    "    TEXT = data.Field(sequential=True, lower=True, include_lengths=True, init_token=\"<SOS>\",eos_token=\"<EOS>\", fix_length=MAX_LEN, batch_first=True)\n",
    "    train, val, test = data.TabularDataset.splits(\n",
    "        path='./data/foruse', train=\"_\".join([\"train\",data_source,feature])+'.tsv',\n",
    "        validation=\"_\".join([\"eval\",data_source,feature])+'.tsv',test=\"_\".join([\"test\",data_source,feature])+'.tsv', \n",
    "        format='tsv',\n",
    "        fields=[('Text', TEXT),])\n",
    "#     TEXT.build_vocab(train, vectors=FastText(language=\"ja\"))\n",
    "    TEXT.build_vocab(train)\n",
    "    TEXT.vocab.set_vectors(stoi=w2v, vectors=vec, dim=300)\n",
    "    train_iter, val_iter, test_iter = data.Iterator.splits((train, val, test), batch_sizes=(BATCH_SIZE, BATCH_SIZE, BATCH_SIZE),sort = False, device=torch.device(device))\n",
    "    vocab_size=TEXT.vocab.vectors.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_conv=nn.Embedding(vocab_size, EMBED_SIZE).to(device)\n",
    "share_conv.weight.data.copy_(TEXT.vocab.vectors)\n",
    "share_conv.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,embed_size,n_hid):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size = embed_size,\n",
    "                            hidden_size = n_hid,\n",
    "                            batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        input: \n",
    "            x = (batch, max_len, embed_dim)\n",
    "        output:\n",
    "            x = (batch, hidden_size)\n",
    "        \"\"\"\n",
    "        _, (h,c)=self.lstm(x) # h=(max_len, batch, n_hid)\n",
    "        return (h,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vae_classifier_2layer(nn.Module):\n",
    "    def __init__(self,n_hid):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(n_hid*2, n_hid)\n",
    "        self.fcmean = nn.Linear(n_hid, n_hid)\n",
    "        self.fcvar = nn.Linear(n_hid, n_hid)\n",
    "        self.ReLU=nn.ReLU()\n",
    "        self.lnorm=nn.LayerNorm(n_hid)\n",
    "\n",
    "    def forward(self, hidden):# h,c = (batch, hidden_size)\n",
    "        h, c = hidden\n",
    "        tmp = self.ReLU(self.lnorm(self.fc(torch.cat([h,c],dim=-1))))\n",
    "        mean = self.fcmean(tmp)\n",
    "        log_sigma_sq = self.fcvar(tmp)\n",
    "        return mean, log_sigma_sq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,embed_size,n_hid,max_len,vocab_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size = embed_size,\n",
    "                            hidden_size = n_hid,\n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(n_hid, vocab_size)\n",
    "        self.embed_size=embed_size\n",
    "        self.max_len=max_len\n",
    "        self.embedding=share_conv\n",
    "        \n",
    "    def forward(self, hidden, x, teacher):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            x_emb=(batch, hidden_size, embedding_size)\n",
    "        output:\n",
    "            logits = (batch, max_len, vocab_size)\n",
    "            sentence = (batch, max_len) : 中身はindex\n",
    "        \"\"\"\n",
    "        logits = torch.tensor([]).to(device)\n",
    "        sentence = torch.tensor([],dtype=torch.long).to(device)\n",
    "        \n",
    "        for i in range(self.max_len):\n",
    "            if torch.rand(1)<1/3:\n",
    "                # word dropout\n",
    "                tmp=torch.zeros(len(x), 1, self.embed_size).to(device)\n",
    "            elif teacher or i==0:\n",
    "                tmp = torch.unsqueeze(x[:,i,:],1) # tmp = (batch, 1, embed_dim)\n",
    "            else:\n",
    "                # word = (batch, 1, 1)\n",
    "                tmp = self.embedding(word) # tmp = (batch, 1, embed_dim)\n",
    "                tmp = F.normalize(tmp)\n",
    "            x_input = tmp # x_input = (batch, 1, (embed_size + n_hid))\n",
    "            out, hidden = self.lstm(x_input, hidden)\n",
    "                # out = (batch, 1, n_hid)\n",
    "                # hidden = ((batch, 1, n_hid),(batch, 1, n_hid))\n",
    "            logit = self.fc(out) # logit = (batch, 1, vocab_size)\n",
    "            word = torch.argmax(logit, dim=2) # word = (batch, 1)\n",
    "\n",
    "            sentence = torch.cat([sentence, word],1)\n",
    "            logits = torch.cat([logits,logit],1)   \n",
    "        return logits, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self,maxlen):\n",
    "        super().__init__()\n",
    "        self.n_hid=LSTM_HIDDEN_SIZE\n",
    "        self.maxlen=maxlen\n",
    "        self.encoder=Encoder(embed_size=EMBED_SIZE,n_hid=self.n_hid)\n",
    "        self.vae_classifer=vae_classifier_2layer(n_hid=self.n_hid)\n",
    "        self.decoder=Decoder(embed_size=EMBED_SIZE,n_hid=self.n_hid,max_len=MAX_LEN,vocab_size=vocab_size)\n",
    "        self.embedding=share_conv\n",
    "        self.fc1 = nn.Linear(self.n_hid, self.n_hid)\n",
    "        self.fc2 = nn.Linear(self.n_hid, self.n_hid)\n",
    "        \n",
    "    def forward(self, x): # x=(batch, max_len)\n",
    "        x_emb=self.embedding(x)  # x_emb = (batch, maxlen, embed_dim)\n",
    "        x_emb = F.normalize(x_emb)\n",
    "        \n",
    "        hidden = self.encoder(x_emb) # h,c = (1, batch, hidden_size)\n",
    "        mean, log_sigma_sq=self.vae_classifer(hidden)\n",
    "        eps = torch.empty(len(x), self.n_hid).normal_(mean=0,std=1).to(device) # N(0, 1)\n",
    "        h = mean + eps * torch.sqrt(torch.exp(log_sigma_sq)) # H_dec = (1, batch, n_gan)\n",
    "        h,c=self.fc1(h), self.fc2(h)\n",
    "        logits, sentence = self.decoder((h,c), x_emb, teacher=True)\n",
    "        \n",
    "        # loss\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=1)\n",
    "        loss = 0\n",
    "        for i in range(self.maxlen-1):\n",
    "            # <SOS>を除くためindexをずらす\n",
    "            loss += criterion(torch.squeeze(logits[:,i,:]), torch.squeeze(x[:,i+1]))\n",
    "        loss/=(self.maxlen-1)\n",
    "        \n",
    "        # KL loss\n",
    "        # 標準正規分布と(μ,σ^2)正規分布を仮定しているので以下の計算式になる\n",
    "        # nn.klDivLossを使うと仮定が甘い\n",
    "        # kl_loss = Σ0.5Σ(μ^2+exp(ln(σ^2))-ln(σ^2)-1)を使う\n",
    "        kl_loss = torch.sum(0.5 * torch.sum((mean**2 + torch.exp(log_sigma_sq) - log_sigma_sq - 1),dim=1))\n",
    "        if epoch>KL_START_EPOCH:\n",
    "            loss += (epoch-KL_START_EPOCH)/(AE_EPOCHS-KL_START_EPOCH)*kl_loss\n",
    "        \n",
    "        return loss, sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_to_sent(sents,j):\n",
    "    word_list=[]\n",
    "    for i in sents[j]:\n",
    "        if i==TEXT.vocab.stoi[\"<EOS>\"]:\n",
    "            break\n",
    "        if i!=TEXT.vocab.stoi[\"<PAD>\"]:\n",
    "            word_list.append(TEXT.vocab.itos[int(i)])\n",
    "    return word_list\n",
    "\n",
    "def write_out(url, origin_sents, syn_sents):\n",
    "    with open(url, \"w\") as f:\n",
    "        for j in range(len(syn_sents)):\n",
    "            f.write(\"input : \"+\" \".join(change_to_sent(origin_sents,j))+\"\\n\")\n",
    "            f.write(\"output: \"+\" \".join(change_to_sent(syn_sents,j))+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    auto_encoder.train()\n",
    "    epoch_loss = 0\n",
    "    count=0\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "        (x, x_l) = batch.Text\n",
    "            # xには文章のID表現が、x_lにはxの単語数が入る\n",
    "            # x=(batch, max_len)\n",
    "#         if len(x)!=BATCH_SIZE:break\n",
    "        optimizer.zero_grad()\n",
    "        loss, syn_sents=auto_encoder(x)        \n",
    "        with torch.autograd.detect_anomaly():\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss+=loss.item()\n",
    "        count+=1\n",
    "        \n",
    "    sample_x=x[0][1:]\n",
    "    source_sentence=' '.join([TEXT.vocab.itos[int(i)] for i in sample_x if i != 1])\n",
    "    gen_sentence=' '.join([TEXT.vocab.itos[int(i)] for i in syn_sents[0] if i != 1])\n",
    "\n",
    "    write_out(log_dir+\"{:03}.txt\".format(epoch), x[:,1:], syn_sents)\n",
    "\n",
    "    history_train.append(epoch_loss/count)\n",
    "    if (epoch+1) % REPORT_INTERVAL==0:\n",
    "        print(\"epoch: \"+str(epoch+1)+'/'+str(AE_EPOCHS)+' ')\n",
    "        print(\"training loss: \"+str(history_train[epoch]))\n",
    "#         print(\"kl_loss: \"+str(kl_loss))\n",
    "        print(\"source(train): \"+str(source_sentence))\n",
    "        print(\"result(train): \"+str(gen_sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start train...\n",
      "epoch: 1/100 \n",
      "training loss: 5.870830535888672\n",
      "source(train): 地球 以外 に 生命 が 発見 さ れ た 事例 は 記録 さ れ て い ない <EOS>\n",
      "result(train): や や 品 石材 語 日本語 れ を を 日本語 こと を を 日本語 について 石材 どの どの にかけて にかけて\n",
      "epoch: 2/100 \n",
      "training loss: 5.4762372970581055\n",
      "source(train): 事物 の 分布 を 考察 する にあたって 分布 図 の 作成 が 挙げ られる <EOS>\n",
      "result(train): この この <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 3/100 \n",
      "training loss: 4.474201202392578\n",
      "source(train): ただし 厳密 な 正書法 は なく 表記 の ゆれ は 広く 許容 さ れ て いる <EOS>\n",
      "result(train): この の <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 4/100 \n",
      "training loss: 4.330178260803223\n",
      "source(train): 年代 により さまざま な 日本語 の 表記 が 行わ れ て いる <EOS>\n",
      "result(train): この この の の の の の の の の <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 5/100 \n",
      "training loss: 4.153068542480469\n",
      "source(train): すなわち 体系 は 下記 の よう に まとめ られ た <EOS>\n",
      "result(train): この は は は は は は は は は <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 6/100 \n",
      "training loss: 4.258059024810791\n",
      "source(train): 一方 形容動詞 は 今日 に 至る まで 高い 造語 力 を 保っ て いる <EOS>\n",
      "result(train): この は は は は は <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 7/100 \n",
      "training loss: 3.996551990509033\n",
      "source(train): パリ は 年間 外国 人 観光 客数 が 世界一 の 観光 都市 で ある <EOS>\n",
      "result(train): この は は は は は は で <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 8/100 \n",
      "training loss: 4.512244701385498\n",
      "source(train): 重要 な 点 は 三つ の 群 で 大きく 異なる が 共通 する こと も 多い <EOS>\n",
      "result(train): この の は は は は は は は は は は は も も <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 9/100 \n",
      "training loss: 4.057750701904297\n",
      "source(train): 重要 な 点 は 三つ の 群 で 大きく 異なる が 共通 する こと も 多い <EOS>\n",
      "result(train): この の の の に で <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 10/100 \n",
      "training loss: 4.097131252288818\n",
      "source(train): パリ は フランス 全土 で も 医師 の 密度 が もっとも 高い 街 の ひとつ で ある <EOS>\n",
      "result(train): この の の に で <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 11/100 \n",
      "training loss: 3.9827635288238525\n",
      "source(train): 逆 に マルタ は 長い 間 アフリカ に 属する 島 と 受け止め られ て い た <EOS>\n",
      "result(train): この の の の の の <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 12/100 \n",
      "training loss: 3.7865355014801025\n",
      "source(train): 言語 と 方言 の 区別 について 現在 なさ れる 説明 は 二つ で ある <EOS>\n",
      "result(train): この は の の の の の <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 13/100 \n",
      "training loss: 3.8519179821014404\n",
      "source(train): 今日 でも この 体系 は 基本 的 に 変わっ て い ない <EOS>\n",
      "result(train): この は の の の の の の の <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 14/100 \n",
      "training loss: 3.7712109088897705\n",
      "source(train): パリ は 年間 外国 人 観光 客数 が 世界一 の 観光 都市 で ある <EOS>\n",
      "result(train): この は は は は は の の の の <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 15/100 \n",
      "training loss: 3.6790308952331543\n",
      "source(train): 対照 的 に 最も 小さな 国家 は バチカン 市 国 で ある <EOS>\n",
      "result(train): この 以外 は は は は は は は の の <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 16/100 \n",
      "training loss: 3.6114420890808105\n",
      "source(train): 品詞 分類 で は 常に 接続 語 と なる 品詞 を 接続詞 と する <EOS>\n",
      "result(train): この は は は は は は の は <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 17/100 \n",
      "training loss: 33585.90234375\n",
      "source(train): 正書法 の 必要 性 を 説く 主張 や その 反論 が しばしば 交わさ れ て き た <EOS>\n",
      "result(train): この は は は は は は の の <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 18/100 \n",
      "training loss: 21438.66015625\n",
      "source(train): 日本語 で は 限り なく 長い 複合語 を 作る こと が 可能 で ある <EOS>\n",
      "result(train): この 語 は は は は の と て <EOS> <EOS> <EOS> <EOS> ある <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 19/100 \n",
      "training loss: 13592.8486328125\n",
      "source(train): ただし 一部 の 方言 に は 今 も 残っ て いる <EOS>\n",
      "result(train): この は は は の は の の で で <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 20/100 \n",
      "training loss: 9202.7744140625\n",
      "source(train): 外国 人 による 日本語 研究 も 中世 末期 から 近世 前期 にかけて 多く 行わ れ た <EOS>\n",
      "result(train): この の は は の の の で の で と で て <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 21/100 \n",
      "training loss: 6249.07568359375\n",
      "source(train): 多く の 場合 において 規則 的 な 対応 が 見 られる <EOS>\n",
      "result(train): この の の の の の に の が ある <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 22/100 \n",
      "training loss: 3337.04052734375\n",
      "source(train): しかし 正確 に 数える こと は ほぼ 不可能 で ある <EOS>\n",
      "result(train): この の の の の に の の が ある <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 23/100 \n",
      "training loss: 1525.7327880859375\n",
      "source(train): しかし 方言 によって は 今 も 開 合 の 区別 が 残っ て いる もの も ある <EOS>\n",
      "result(train): この の の は 変化 の に の の の の ある て <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 24/100 \n",
      "training loss: 989.6011352539062\n",
      "source(train): また 社会 心理 学 や 小 集団 実験 と 関連 する 研究 も 多い <EOS>\n",
      "result(train): に <EOS> で と <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 25/100 \n",
      "training loss: 1184.043701171875\n",
      "source(train): この 状態 で 虫 が つく こと も ほとんど 無い と 言う <EOS>\n",
      "result(train): 設置 で で <EOS> が ある <EOS> が ある <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 26/100 \n",
      "training loss: 1488.48486328125\n",
      "source(train): 日本語 は 主 に 日本 国内 で 使用 さ れる <EOS>\n",
      "result(train): すなわち 他 こと の ある で で ある で れ <EOS> <EOS> <EOS> の の の の の の の\n",
      "epoch: 27/100 \n",
      "training loss: 1532.3681640625\n",
      "source(train): それ 以外 と なる と かなり 重要 度 が 落ちる <EOS>\n",
      "result(train): これら で で ある <EOS> <EOS> の で が <EOS> <EOS> の の の の の の の の の\n",
      "epoch: 28/100 \n",
      "training loss: 1282.3165283203125\n",
      "source(train): 一つ の 方言 区画 の 内部 も 変化 に 富ん で いる <EOS>\n",
      "result(train): いく 他 は の は は の は の の の ある <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 29/100 \n",
      "training loss: 1058.3359375\n",
      "source(train): ことに 述語 は 文 を まとめる 重要 な 役割 を 果たす <EOS>\n",
      "result(train): 採る <EOS> <EOS> <EOS> を 採る <EOS> <EOS> <EOS> で ある <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 30/100 \n",
      "training loss: 903.5757446289062\n",
      "source(train): もともと 少ない 形容詞 を 補う 主要 な 形式 は 形容動詞 で ある <EOS>\n",
      "result(train): いく <SOS> 事物 は は は の ある は ある <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 31/100 \n",
      "training loss: 611.2518310546875\n",
      "source(train): 一つ の 方言 区画 の 内部 も 変化 に 富ん で いる <EOS>\n",
      "result(train): この の は は は 現在 の 変化 は 今 の ある <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 32/100 \n",
      "training loss: 539.0488891601562\n",
      "source(train): この 状態 で 虫 が つく こと も ほとんど 無い と 言う <EOS>\n",
      "result(train): この 以外 は は は は ため の それぞれ の <EOS> ある <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 33/100 \n",
      "training loss: 667.2766723632812\n",
      "source(train): 数 多く の 病院 が パリ に 設置 さ れ て いる <EOS>\n",
      "result(train): この は は は は それぞれ に 設置 て れ <EOS> いる <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 34/100 \n",
      "training loss: 600.1408081054688\n",
      "source(train): しかし 方言 によって は 今 も 開 合 の 区別 が 残っ て いる もの も ある <EOS>\n",
      "result(train): ただし で の <EOS> ある <EOS> ある <EOS> ある <EOS> <EOS> <EOS> て いる <EOS> も ある <EOS> <EOS> <EOS>\n",
      "epoch: 35/100 \n",
      "training loss: 405.68365478515625\n",
      "source(train): この 川 の 中州 で ある シテ 島 を 中心 に 発達 し た <EOS>\n",
      "result(train): ただし で で 現在 で ある <EOS> <EOS> <EOS> <EOS> <EOS> ある で た <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 36/100 \n",
      "training loss: 371.88134765625\n",
      "source(train): この 種 の 批判 は 古典 文学 の 中 に も 見 られる <EOS>\n",
      "result(train): 分類 は を 現在 の ある を を ある <EOS> ある <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 37/100 \n",
      "training loss: 451.1177673339844\n",
      "source(train): ただし 厳密 な 正書法 は なく 表記 の ゆれ は 広く 許容 さ れ て いる <EOS>\n",
      "result(train): で も ある <EOS> の ある <EOS> <EOS> ある <EOS> ある で が れ て <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 38/100 \n",
      "training loss: 401.4049987792969\n",
      "source(train): 一方 形容動詞 は 今日 に 至る まで 高い 造語 力 を 保っ て いる <EOS>\n",
      "result(train): ただし 以外 は 現在 に ある て れ て の ある <EOS> て いる <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 39/100 \n",
      "training loss: 278.1938781738281\n",
      "source(train): 語 によって は 特定 の 尊敬 語 が 対応 する もの も ある <EOS>\n",
      "result(train): ただし 語 は 現在 の が する として ある の <EOS> も ある <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 40/100 \n",
      "training loss: 229.78843688964844\n",
      "source(train): 第 二 の もの は まず どの 方言 も それぞれ 言語 だ と する <EOS>\n",
      "result(train): に の <EOS> <EOS> も ある の な の ある られる の ある <EOS> <EOS> も ある <EOS> <EOS> <EOS>\n",
      "epoch: 41/100 \n",
      "training loss: 246.1031494140625\n",
      "source(train): ただし 海水 中 に 生育 する もの は 確認 さ れ て い ない <EOS>\n",
      "result(train): て の <EOS> <EOS> ある する <EOS> も ある <EOS> れ て いる <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 42/100 \n",
      "training loss: 252.4849395751953\n",
      "source(train): 移民 の 居住 区域 は それぞれ 出身 地 ごと に 異なっ て いる <EOS>\n",
      "result(train): ただし は は は の ある 出身 <EOS> <EOS> <EOS> <EOS> て <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 43/100 \n",
      "training loss: 230.31507873535156\n",
      "source(train): すなわち 体系 は 下記 の よう に まとめ られ た <EOS>\n",
      "result(train): ただし 記号 は は の は に 設置 られ <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 44/100 \n",
      "training loss: 174.22003173828125\n",
      "source(train): パリ 大学 は 特に 神学 の 研究 で 著名 で あっ た <EOS>\n",
      "result(train): ただし は は は に の は の ある で ある た <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 45/100 \n",
      "training loss: 129.74871826171875\n",
      "source(train): 特に 移民 や 植民 地 など で フランス 色 が 強い 都市 に 多い <EOS>\n",
      "result(train): 北 ただし 以外 は は は は <EOS> に が ある 都市 で ある <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 46/100 \n",
      "training loss: 147.540771484375\n",
      "source(train): 一つ の 方言 区画 の 内部 も 変化 に 富ん で いる <EOS>\n",
      "result(train): パリ の は によって は は は ある <EOS> ある て ある <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 47/100 \n",
      "training loss: 168.35055541992188\n",
      "source(train): 年代 により さまざま な 日本語 の 表記 が 行わ れ て いる <EOS>\n",
      "result(train): この の は は な は 現在 で ある れる て <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 48/100 \n",
      "training loss: 134.84188842773438\n",
      "source(train): 日本語 で は 限り なく 長い 複合語 を 作る こと が 可能 で ある <EOS>\n",
      "result(train): この の は は なく 長い 複合語 と は <EOS> が ある で ある <EOS> <EOS> ある <EOS> <EOS> <EOS>\n",
      "epoch: 49/100 \n",
      "training loss: 97.31373596191406\n",
      "source(train): パリ 市 は 県庁 所在地 と さ れ て い た <EOS>\n",
      "result(train): 地理 は は 現在 所在地 で なる て <EOS> いる ない <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 50/100 \n",
      "training loss: 91.01953125\n",
      "source(train): 語 によって は 特定 の 尊敬 語 が 対応 する もの も ある <EOS>\n",
      "result(train): その の の は の よう 語 と する て <EOS> も ある <EOS> も ある <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 51/100 \n",
      "training loss: 94.8057861328125\n",
      "source(train): 語族 は さらに 語 派 語 群 そして 言語 と 細分 化 さ れ て いく <EOS>\n",
      "result(train): それ は は の と の の で <EOS> と する する <EOS> れ て いる <EOS> <EOS> ある <EOS>\n",
      "epoch: 52/100 \n",
      "training loss: 98.25534057617188\n",
      "source(train): その ため 何 百 年 という 間 に は 話し言葉 と 書き言葉 の 差 が 生まれる <EOS>\n",
      "result(train): この の は は 年 の 間 に 存在 て と する の 区別 が れ <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 53/100 \n",
      "training loss: 81.99087524414062\n",
      "source(train): そうした 言い方 を 習得 する こと は どの 言語 でも 容易 で ない <EOS>\n",
      "result(train): この の は は する の の 今 な は 各地 で ある <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 54/100 \n",
      "training loss: 57.18757629394531\n",
      "source(train): この 川 の 中州 で ある シテ 島 を 中心 に 発達 し た <EOS>\n",
      "result(train): また の の は で は もの 島 で 提供 する <EOS> し ある <EOS> <EOS> ある <EOS> <EOS> <EOS>\n",
      "epoch: 55/100 \n",
      "training loss: 59.694374084472656\n",
      "source(train): 一方 形容動詞 は 今日 に 至る まで 高い 造語 力 を 保っ て いる <EOS>\n",
      "result(train): この の は は に 設置 まで 何 造語 力 ある <EOS> て <EOS> <EOS> も ある <EOS> <EOS> <EOS>\n",
      "epoch: 56/100 \n",
      "training loss: 66.86839294433594\n",
      "source(train): また ほか の 高等 教育 機関 も この 地区 に 存在 する <EOS>\n",
      "result(train): この は の 日本語 の は も ある 地区 の 存在 する <EOS> も ある <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 57/100 \n",
      "training loss: 59.4642333984375\n",
      "source(train): 主 な 格 助詞 と その 典型 的 な 機能 は 次 の 通り で ある <EOS>\n",
      "result(train): この は 点 は に する 典型 的 な 機能 は ある の 通り で ある <EOS> <EOS> ある <EOS>\n",
      "epoch: 58/100 \n",
      "training loss: 48.002685546875\n",
      "source(train): 品詞 分類 で は 常に 接続 語 と なる 品詞 を 接続詞 と する <EOS>\n",
      "result(train): この の で は 今 接続 語 として する さ は 接続詞 する <EOS> <EOS> も ある <EOS> <EOS> <EOS>\n",
      "epoch: 59/100 \n",
      "training loss: 37.651859283447266\n",
      "source(train): 自立 語 で 活用 の ない もの の うち 主語 に なる もの を 名詞 と する <EOS>\n",
      "result(train): これ 語 として 活用 の は が が 主語 主語 ある <EOS> <EOS> と 列挙 する する <EOS> も ある\n",
      "epoch: 60/100 \n",
      "training loss: 41.67848205566406\n",
      "source(train): 言語 と 方言 の 区別 について 現在 なさ れる 説明 は 二つ で ある <EOS>\n",
      "result(train): この 以外 は によって 区別 が の なさ れ て て ない で ある <EOS> も ある <EOS> <EOS> <EOS>\n",
      "epoch: 61/100 \n",
      "training loss: 43.60400390625\n",
      "source(train): 有機物 以外 を 構成 要素 と する 生物 も 想定 さ れる <EOS>\n",
      "result(train): この 以外 に は 要素 と する する と 見 さ れ <EOS> も ある <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 62/100 \n",
      "training loss: 33.767337799072266\n",
      "source(train): パリ 市内 は 現在 も 大学 の 中心 地 で あり 続け て いる <EOS>\n",
      "result(train): ただし 市 は 日本語 も 大学 の 中心 地 で で 続け <EOS> いる <EOS> も ある <EOS> <EOS> <EOS>\n",
      "epoch: 63/100 \n",
      "training loss: 27.5904598236084\n",
      "source(train): 品詞 分類 で は 常に 接続 語 と なる 品詞 を 接続詞 と する <EOS>\n",
      "result(train): この の は は 今 の の と なる 品詞 も ある と する <EOS> も ある <EOS> <EOS> <EOS>\n",
      "epoch: 64/100 \n",
      "training loss: 27.357206344604492\n",
      "source(train): パリ 市内 は 現在 も 大学 の 中心 地 で あり 続け て いる <EOS>\n",
      "result(train): 品詞 で は 現在 の それぞれ の よう に で ある <EOS> <EOS> いる <EOS> も ある <EOS> <EOS> <EOS>\n",
      "epoch: 65/100 \n",
      "training loss: 29.963911056518555\n",
      "source(train): 日本語 は 主 に 日本 国内 で 使用 さ れる <EOS>\n",
      "result(train): この の 音読み に 社会 国内 で 使用 の の に <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 66/100 \n",
      "training loss: 24.4628963470459\n",
      "source(train): パリ 市 は 県庁 所在地 と さ れ て い た <EOS>\n",
      "result(train): この は は 現在 所在地 と いう れる て いる ない <EOS> <EOS> ある <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 67/100 \n",
      "training loss: 20.190414428710938\n",
      "source(train): 言語 と 方言 の 区別 について 現在 なさ れる 説明 は 二つ で ある <EOS>\n",
      "result(train): ただし の の 区画 区別 について 開 なさ れる 説明 も 二つ で ある <EOS> <EOS> ある <EOS> <EOS> <EOS>\n",
      "epoch: 68/100 \n",
      "training loss: 18.793685913085938\n",
      "source(train): その後 今日 に 至る まで 何 度 か 移民 の 波 が 続い て いる <EOS>\n",
      "result(train): 漢語 の に は まで 何 は か 移民 の 波 が 強い で いる <EOS> も ある <EOS> <EOS>\n",
      "epoch: 69/100 \n",
      "training loss: 18.809051513671875\n",
      "source(train): しかし 実際 の 言語 生活 に 照らし て 考えれ ば これ は 事実 で は ない <EOS>\n",
      "result(train): この 正確 の 日本語 生活 は 表記 て 考えれ ば 分かり <EOS> 事実 で ある ない <EOS> ある <EOS> <EOS>\n",
      "epoch: 70/100 \n",
      "training loss: 16.624805450439453\n",
      "source(train): パリ 市内 は 現在 も 大学 の 中心 地 で あり 続け て いる <EOS>\n",
      "result(train): この 市内 は 県庁 も 大学 の 中心 地 で あり 同国 て いる <EOS> も ある <EOS> <EOS> <EOS>\n",
      "epoch: 71/100 \n",
      "training loss: 13.871819496154785\n",
      "source(train): ただし ナ 変 は 近代 に 入っ て も なお 使用 さ れる こと が あっ た <EOS>\n",
      "result(train): 日本語 は 変 は 日本語 の 設置 て いる なお も さ れる <EOS> が ある た <EOS> <EOS> ある\n",
      "epoch: 72/100 \n",
      "training loss: 12.915863037109375\n",
      "source(train): 人間 は 歴史 社会 を 創造 する とともに 歴史 社会 の なか を 生きる 存在 で ある <EOS>\n",
      "result(train): この は は 社会 を 創造 する とともに も ある を なか を 生きる 存在 で ある <EOS> <EOS> ある\n",
      "epoch: 73/100 \n",
      "training loss: 13.010859489440918\n",
      "source(train): また ほか の 高等 教育 機関 も この 地区 に 存在 する <EOS>\n",
      "result(train): この の の 日本語 の 機関 も 少なく 地区 に ある <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 74/100 \n",
      "training loss: 12.313557624816895\n",
      "source(train): パリ は フランス 全土 で も 医師 の 密度 が もっとも 高い 街 の ひとつ で ある <EOS>\n",
      "result(train): その 市 年間 全土 は も の の 方 が もっとも て 街 の ひとつ で ある <EOS> <EOS> <EOS>\n",
      "epoch: 75/100 \n",
      "training loss: 9.67746639251709\n",
      "source(train): つまり 同じ 地方 の コミュニティ 内 で 通用 する 言語 を 使用 し て いる <EOS>\n",
      "result(train): この の 地方 の コミュニティ 内 で 通用 する こと も 使用 し て いる <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 76/100 \n",
      "training loss: 10.071310043334961\n",
      "source(train): 文 は 目的 や 場面 など に 応じ て さまざま な 異なっ た 様式 を 採る <EOS>\n",
      "result(train): この を 目的 や 場面 など に 強い さ いる <EOS> <EOS> た <EOS> を 採る する <EOS> <EOS> <EOS>\n",
      "epoch: 77/100 \n",
      "training loss: 9.91053295135498\n",
      "source(train): 今日 でも この 体系 は 基本 的 に 変わっ て い ない <EOS>\n",
      "result(train): この の は 日本語 は 基本 の よう も の いる ない <EOS> ある <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 78/100 \n",
      "training loss: 8.326448440551758\n",
      "source(train): ただし 海水 中 に 生育 する もの は 確認 さ れ て い ない <EOS>\n",
      "result(train): この 一部 の 日本語 生育 する ため は 確認 さ れ て い <EOS> <EOS> <EOS> 記録 <EOS> <EOS> <EOS>\n",
      "epoch: 79/100 \n",
      "training loss: 6.829043865203857\n",
      "source(train): 文 は 目的 や 場面 など に 応じ て さまざま な 異なっ た 様式 を 採る <EOS>\n",
      "result(train): この を 目的 や 場面 など に 応じ て いる な 異なっ た 様式 を 採る <EOS> <EOS> ある <EOS>\n",
      "epoch: 80/100 \n",
      "training loss: 7.15294075012207\n",
      "source(train): 漢語 の 勢力 は 今日 まで 拡大 を 続け て いる <EOS>\n",
      "result(train): この は 勢力 は 今日 まで 拡大 を 続け て いる いる <EOS> <EOS> <EOS> <EOS> た た た た\n",
      "epoch: 81/100 \n",
      "training loss: 6.85764217376709\n",
      "source(train): この 他 教育 学部 に 設置 さ れ て いる 大学 も ある <EOS>\n",
      "result(train): この 種 教育 学部 の 設置 さ れ て いる <EOS> も ある <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 82/100 \n",
      "training loss: 5.925444602966309\n",
      "source(train): この 説明 方法 は 現在 の 学校 教育 の 国語 でも 取り入れ られ て いる <EOS>\n",
      "result(train): 文 うち の は 現在 的 群 が の 国語 でも と られ て いる <EOS> <EOS> ある <EOS> <EOS>\n",
      "epoch: 83/100 \n",
      "training loss: 5.541499137878418\n",
      "source(train): また 社会 心理 学 や 小 集団 実験 と 関連 する 研究 も 多い <EOS>\n",
      "result(train): その の 心理 は や 小 集団 実験 実験 関連 する 研究 も 多い <EOS> <EOS> ある <EOS> <EOS> <EOS>\n",
      "epoch: 84/100 \n",
      "training loss: 6.411273956298828\n",
      "source(train): ただし ナ 変 は 近代 に 入っ て も なお 使用 さ れる こと が あっ た <EOS>\n",
      "result(train): 特に 的 は は は に に に いる なお ある ある れる <EOS> が あっ <EOS> <EOS> <EOS> ある\n",
      "epoch: 85/100 \n",
      "training loss: 4.846572399139404\n",
      "source(train): 言語 と 方言 の 区別 について 現在 なさ れる 説明 は 二つ で ある <EOS>\n",
      "result(train): この と 方言 の 区別 について 現在 なさ れる 説明 は 二つ で ある <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 86/100 \n",
      "training loss: 3.886120557785034\n",
      "source(train): 漢語 は 音読み で 読ま れる こと から 字音 語 と 呼ば れる 場合 も ある <EOS>\n",
      "result(train): この の 音読み で 読ま れる こと から 字音 語 と 呼ば れる 場合 も ある <EOS> <EOS> ある <EOS>\n",
      "epoch: 87/100 \n",
      "training loss: 4.197870254516602\n",
      "source(train): この うち 最も 規模 の 大きな 変化 は 二 段 活用 の 一段 化 で ある <EOS>\n",
      "result(train): この うち とりわけ 規模 の 大きな を 国 バチカン 段 活用 の 一段 化 で ある <EOS> <EOS> ある <EOS>\n",
      "epoch: 88/100 \n",
      "training loss: 4.869138717651367\n",
      "source(train): 人間 は 歴史 社会 を 創造 する とともに 歴史 社会 の なか を 生きる 存在 で ある <EOS>\n",
      "result(train): この 語 は は 学 創造 する とともに 分布 多い <EOS> <EOS> を 生きる 存在 で ある <EOS> <EOS> <EOS>\n",
      "epoch: 89/100 \n",
      "training loss: 3.4291858673095703\n",
      "source(train): 移民 の 居住 区域 は それぞれ 出身 地 ごと に 異なっ て いる <EOS>\n",
      "result(train): この の 居住 区域 は それぞれ 出身 地 ごと に 異なっ て いる <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 90/100 \n",
      "training loss: 3.633181095123291\n",
      "source(train): つまり 同じ 地方 の コミュニティ 内 で 通用 する 言語 を 使用 し て いる <EOS>\n",
      "result(train): 古代 同じ 地方 の コミュニティ が で 通用 する 言語 を 使用 し て いる <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 91/100 \n",
      "training loss: 4.032312870025635\n",
      "source(train): 文語 の 終止 形 が 化石 的 に 残っ て いる 場合 も ある <EOS>\n",
      "result(train): この の 終止 形 は 化石 的 から 残っ て いる <EOS> も ある <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 92/100 \n",
      "training loss: 3.626006603240967\n",
      "source(train): しかし 正確 に 数える こと は ほぼ 不可能 で ある <EOS>\n",
      "result(train): この は は 数える こと は どの 不可能 と ある <EOS> <EOS> ある <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 93/100 \n",
      "training loss: 2.9121947288513184\n",
      "source(train): エジプト の 主要 交易 品 と 言え ば 金 で あっ た <EOS>\n",
      "result(train): この の 主要 交易 品 と 言え ば 金 で あっ た <EOS> が ある <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 94/100 \n",
      "training loss: 3.04580020904541\n",
      "source(train): 自立 語 で 活用 の ない もの の うち 主語 に なる もの を 名詞 と する <EOS>\n",
      "result(train): この 語 で 活用 の ない もの と 活用 に に なる もの を 名詞 と する <EOS> <EOS> <EOS>\n",
      "epoch: 95/100 \n",
      "training loss: 2.9540834426879883\n",
      "source(train): この ギャル 文字 を 練習 する ため の 本 も 現れ た <EOS>\n",
      "result(train): この の 文字 を 練習 する ため の 本 で 出版 た <EOS> が ある <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 96/100 \n",
      "training loss: 2.7008910179138184\n",
      "source(train): 漢語 は 音読み で 読ま れる こと から 字音 語 と 呼ば れる 場合 も ある <EOS>\n",
      "result(train): この の 音読み で 読ま れる こと から あっ た も なる れる <EOS> も ある <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 97/100 \n",
      "training loss: 2.4078667163848877\n",
      "source(train): 古代 エジプト は 次 の 時代 に 区分 さ れ て いる <EOS>\n",
      "result(train): その エジプト は 次 の 時代 に 区分 さ れ て いる <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 98/100 \n",
      "training loss: 2.9919989109039307\n",
      "source(train): この うち 最も 規模 の 大きな 変化 は 二 段 活用 の 一段 化 で ある <EOS>\n",
      "result(train): この 状態 最も 規模 の 大きな を は 二 段 波 の ある 化 で ある <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 99/100 \n",
      "training loss: 2.669069766998291\n",
      "source(train): 季節 による 雨量 の 変化 も 少なく ほぼ 一定 し て いる <EOS>\n",
      "result(train): その の 雨量 は コミュニティ も 開 ほぼ 一定 し て いる <EOS> も ある <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 100/100 \n",
      "training loss: 2.260401725769043\n",
      "source(train): ことに 述語 は 文 を まとめる 重要 な 役割 を 果たす <EOS>\n",
      "result(train): 日本語 の は 文 を まとめる 重要 な 役割 を 果たす <EOS> <EOS> ある <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "学習を始める\n",
    "'''\n",
    "print(\"start train...\")\n",
    "auto_encoder=AutoEncoder(maxlen=MAX_LEN)\n",
    "auto_encoder.to(device)\n",
    "optimizer = optim.Adam(auto_encoder.parameters(), lr=ae_lr)\n",
    "history_train=[]\n",
    "history_eval=[]\n",
    "\n",
    "for epoch in range(AE_EPOCHS):\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [i for i in range(len(history_train))]\n",
    "plt.plot(x, history_train, label=\"train\")\n",
    "# plt.plot(x, history_eval, label=\"eval\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttext確認用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if torch.rand(1)<1/3:\n",
    "    print(\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nihongo_id=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「その」の似ている単語\n",
      "この:tensor(0.7487, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "の:tensor(0.6892, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "が:tensor(0.6426, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "た:tensor(0.6121, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "それ:tensor(0.6105, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "これ:tensor(0.6037, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "も:tensor(0.5796, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "を:tensor(0.5789, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "しかし:tensor(0.5774, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "に:tensor(0.5679, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "nihongo_id+=1\n",
    "norm_list=[]\n",
    "word=TEXT.vocab.itos[nihongo_id]\n",
    "nihongo_vector=share_conv.weight[nihongo_id]\n",
    "for i, val in enumerate(share_conv.weight):\n",
    "    cos=torch.nn.CosineSimilarity(dim=0)\n",
    "    diff=cos(nihongo_vector,val)\n",
    "    norm=torch.norm(diff)\n",
    "    norm_list.append((norm,i))\n",
    "norm_list.sort(reverse=True)\n",
    "print(\"「\"+word+\"」の似ている単語\")\n",
    "for i in range(1,11):\n",
    "    val=norm_list[i][0]\n",
    "    id_=norm_list[i][1]\n",
    "    print(TEXT.vocab.itos[id_]+\":\"+str(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86188\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/foruse/\"+\"_\".join([\"train\",data_source,feature])+'.tsv') as f:\n",
    "    c=0\n",
    "    for i in f:\n",
    "        c+=1\n",
    "print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36-yasuda",
   "language": "python",
   "name": "py36-yasuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
