{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data, datasets\n",
    "from torchtext.vocab import FastText\n",
    "\n",
    "import os\n",
    "import math\n",
    "import MeCab\n",
    "\n",
    "device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 1\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "mecab = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "# BLEU\n",
    "\n",
    "from Util.selfbleu import CalcSelfBLEU\n",
    "from Util.bleu import calc_all_bleu\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPER PARAMETER\n",
    "BATCH_SIZE=100\n",
    "EMBED_SIZE=300\n",
    "LSTM_HIDDEN_SIZE=300\n",
    "AE_EPOCHS=100\n",
    "REPORT_INTERVAL=5\n",
    "ae_lr=1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL NAME\n",
    "data_source=\"wiki\"\n",
    "feature=\"only_ja_small\"\n",
    "feature=\"only_ja_sample\"\n",
    "# feature=\"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_source==\"wiki\" or data_source==\"orphans\":\n",
    "    MAX_LEN=20 #paddingをバッチにかかわらず固定長にする\n",
    "    TEXT = data.Field(sequential=True, lower=True, include_lengths=True, init_token=\"<SOS>\",eos_token=\"<EOS>\", fix_length=MAX_LEN, batch_first=True)\n",
    "    train, val, test = data.TabularDataset.splits(\n",
    "        path='./data/foruse', train=\"_\".join([\"train\",data_source,feature])+'.tsv',\n",
    "        validation=\"_\".join([\"eval\",data_source,feature])+'.tsv',test=\"_\".join([\"test\",data_source,feature])+'.tsv', \n",
    "        format='tsv',\n",
    "        fields=[('Text', TEXT),])\n",
    "    TEXT.build_vocab(train, vectors=FastText(language=\"ja\"))\n",
    "    train_iter, val_iter, test_iter = data.Iterator.splits((train, val, test), batch_sizes=(BATCH_SIZE, BATCH_SIZE, BATCH_SIZE),sort = False, device=torch.device(device))\n",
    "    vocab_size=TEXT.vocab.vectors.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_conv=nn.Embedding(vocab_size, EMBED_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,embed_size,n_hid):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size = embed_size,\n",
    "                            hidden_size = n_hid,\n",
    "                            batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        input: \n",
    "            x = (batch, max_len, embed_dim)\n",
    "        output:\n",
    "            x = (batch, hidden_size)\n",
    "        \"\"\"\n",
    "        _, (h,c)=self.lstm(x) # h=(max_len, batch, n_hid)\n",
    "        return (h,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,embed_size,n_hid,max_len,vocab_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size = embed_size,\n",
    "                            hidden_size = n_hid,\n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(n_hid, vocab_size)\n",
    "        self.lnorm=nn.LayerNorm(n_hid)\n",
    "        self.max_len=max_len\n",
    "        self.embedding=share_conv\n",
    "        \n",
    "    def forward(self, hidden, x, teacher):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            x_emb=(batch, hidden_size, embedding_size)\n",
    "        output:\n",
    "            logits = (batch, max_len, vocab_size)\n",
    "            sentence = (batch, max_len) : 中身はindex\n",
    "        \"\"\"\n",
    "        logits = torch.tensor([]).to(device)\n",
    "        sentence = torch.tensor([],dtype=torch.long).to(device)\n",
    "        \n",
    "        for i in range(self.max_len):\n",
    "            if teacher or i==0:\n",
    "                tmp = torch.unsqueeze(x[:,i,:],1) # tmp = (batch, 1, embed_dim)\n",
    "            else:\n",
    "                # word = (batch, 1, 1)\n",
    "                tmp = self.embedding(word) # tmp = (batch, 1, embed_dim)\n",
    "            x_input = tmp # x_input = (batch, 1, (embed_size + n_hid))\n",
    "            out, hidden = self.lstm(x_input, hidden)\n",
    "                # out = (batch, 1, n_hid)\n",
    "                # hidden = ((batch, 1, n_hid),(batch, 1, n_hid))\n",
    "            logit = self.fc(out) # logit = (batch, 1, vocab_size)\n",
    "            word = torch.argmax(logit, dim=2) # word = (batch, 1)\n",
    "\n",
    "            sentence = torch.cat([sentence, word],1)\n",
    "            logits = torch.cat([logits,logit],1)   \n",
    "        return logits, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self,maxlen):\n",
    "        super().__init__()\n",
    "        self.maxlen=maxlen\n",
    "        self.encoder=Encoder(embed_size=EMBED_SIZE,n_hid=LSTM_HIDDEN_SIZE)\n",
    "        self.decoder=Decoder(embed_size=EMBED_SIZE,n_hid=LSTM_HIDDEN_SIZE,max_len=MAX_LEN,vocab_size=vocab_size)\n",
    "        self.embedding=share_conv\n",
    "        \n",
    "    def forward(self, x): # x=(batch, max_len)\n",
    "        x_emb=self.embedding(x)  # x_emb = (batch, maxlen, embed_dim)\n",
    "        \n",
    "        hidden = self.encoder(x_emb) # h,c = (batch, hidden_size)\n",
    "        logits, sentence = self.decoder(hidden, x_emb, teacher=True)\n",
    "        \n",
    "        # loss\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=1)\n",
    "        loss = 0\n",
    "        for i in range(self.maxlen-1):\n",
    "            # <SOS>を除くためindexをずらす\n",
    "            loss += criterion(torch.squeeze(logits[:,i,:]), torch.squeeze(x[:,i+1]))\n",
    "        loss/=(self.maxlen-1)\n",
    "        \"\"\"\n",
    "        # KL loss\n",
    "        # 標準正規分布と(μ,σ^2)正規分布を仮定しているので以下の計算式になる\n",
    "        # nn.klDivLossを使うと仮定が甘い\n",
    "        # kl_loss = Σ0.5Σ(μ^2+exp(ln(σ^2))-ln(σ^2)-1)を使う\n",
    "        kl_loss = torch.sum(0.5 * torch.sum((H_mean**2 + torch.exp(H_log_sigma_sq) - H_log_sigma_sq - 1),dim=1))\n",
    "        loss += epoch/ae_epoch_number*kl_loss\n",
    "        \"\"\"\n",
    "        \n",
    "        return loss, sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    auto_encoder.train()\n",
    "    epoch_loss = 0\n",
    "    count=0\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "        (x, x_l) = batch.Text\n",
    "            # xには文章のID表現が、x_lにはxの単語数が入る\n",
    "            # x=(batch, max_len)\n",
    "        if len(x)!=BATCH_SIZE:break\n",
    "        optimizer.zero_grad()\n",
    "        loss, syn_sents=auto_encoder(x)\n",
    "        with torch.autograd.detect_anomaly():\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss+=loss.item()\n",
    "        count+=1\n",
    "    sample_x=x[0][1:]\n",
    "    source_sentence=' '.join([TEXT.vocab.itos[int(i)] for i in sample_x if i != 1])\n",
    "    gen_sentence=' '.join([TEXT.vocab.itos[int(i)] for i in syn_sents[0] if i != 1])\n",
    "    history_train.append(epoch_loss/count)\n",
    "    if (epoch+1) % REPORT_INTERVAL==0:\n",
    "        print(\"epoch: \"+str(epoch+1)+'/'+str(AE_EPOCHS)+' ')\n",
    "        print(\"training loss: \"+str(history_train[epoch]))\n",
    "#         print(\"kl_loss: \"+str(kl_loss))\n",
    "        print(\"source(train): \"+str(source_sentence))\n",
    "        print(\"result(train): \"+str(gen_sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start train...\n",
      "epoch: 5/100 \n",
      "training loss: 3.064174175262451\n",
      "source(train): 数 多く の 病院 が パリ に 設置 さ れ て いる <EOS>\n",
      "result(train): この の の で で で ある 存在 て れ て いる <EOS> も <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 10/100 \n",
      "training loss: 1.3314015865325928\n",
      "source(train): 表記 体系 は ほか の 諸 言語 と 比べ て 複雑 で ある <EOS>\n",
      "result(train): この の は 現在 の 区別 言語 に 比べ て いる で ある <EOS> は <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 15/100 \n",
      "training loss: 0.4283473491668701\n",
      "source(train): 地理 学 誕生 の 地 は 古代 ギリシア で ある <EOS>\n",
      "result(train): この 学 誕生 の 地 は 古代 ギリシア で ある <EOS> は を を 語 語 語 語 語 語\n",
      "epoch: 20/100 \n",
      "training loss: 0.1961958110332489\n",
      "source(train): 言語 と 方言 の 区別 について 現在 なさ れる 説明 は 二つ で ある <EOS>\n",
      "result(train): この と 方言 の 区別 について 現在 なさ れる 説明 は 二つ で ある <EOS> は が を を を\n",
      "epoch: 25/100 \n",
      "training loss: 0.11613862216472626\n",
      "source(train): これら 化学 反応 が おこる 場 を 提供 し て いる の が 水 で ある <EOS>\n",
      "result(train): これら 化学 反応 が おこる 場 を 提供 し て いる の が 水 で ある <EOS> も が を\n",
      "epoch: 30/100 \n",
      "training loss: 0.06710708886384964\n",
      "source(train): この 他 教育 学部 に 設置 さ れ て いる 大学 も ある <EOS>\n",
      "result(train): この 他 教育 学部 に 設置 さ れ て いる 大学 も ある <EOS> は は を を を を\n",
      "epoch: 35/100 \n",
      "training loss: 0.03503086045384407\n",
      "source(train): 人間 は 歴史 社会 を 創造 する とともに 歴史 社会 の なか を 生きる 存在 で ある <EOS>\n",
      "result(train): 人間 は 歴史 社会 を 創造 する とともに 歴史 社会 の なか を 生きる 存在 で ある <EOS> ある ある\n",
      "epoch: 40/100 \n",
      "training loss: 0.016545012593269348\n",
      "source(train): また 街 の 予算 は 国 の 同意 を 得る 必要 が あっ た <EOS>\n",
      "result(train): また 街 の 予算 は 国 の 同意 を 得る 必要 が あっ た <EOS> は <EOS> <EOS> <EOS> を\n",
      "epoch: 45/100 \n",
      "training loss: 0.008062309585511684\n",
      "source(train): 正書法 の 必要 性 を 説く 主張 や その 反論 が しばしば 交わさ れ て き た <EOS>\n",
      "result(train): 正書法 の 必要 性 を 説く 主張 や その 反論 が しばしば 交わさ れ て き た <EOS> は <EOS>\n",
      "epoch: 50/100 \n",
      "training loss: 0.004223538096994162\n",
      "source(train): エジプト の 主要 交易 品 と 言え ば 金 で あっ た <EOS>\n",
      "result(train): エジプト の 主要 交易 品 と 言え ば 金 で あっ た <EOS> は <EOS> <EOS> を を を を\n",
      "epoch: 55/100 \n",
      "training loss: 0.002584397094324231\n",
      "source(train): しかし 正確 に 数える こと は ほぼ 不可能 で ある <EOS>\n",
      "result(train): しかし 正確 に 数える こと は ほぼ 不可能 で ある <EOS> は が を を を を を を を\n",
      "epoch: 60/100 \n",
      "training loss: 0.0017765350639820099\n",
      "source(train): 有機物 以外 を 構成 要素 と する 生物 も 想定 さ れる <EOS>\n",
      "result(train): 有機物 以外 を 構成 要素 と する 生物 も 想定 さ れる <EOS> も <EOS> <EOS> <EOS> その その その\n",
      "epoch: 65/100 \n",
      "training loss: 0.0013641592813655734\n",
      "source(train): 日本語 が 時 と共に 変化 する こと は しばしば 批判 の 対象 と なる <EOS>\n",
      "result(train): 日本語 が 時 と共に 変化 する こと は しばしば 批判 の 対象 と なる <EOS> は <EOS> <EOS> <EOS> <EOS>\n",
      "epoch: 70/100 \n",
      "training loss: 0.0011251146206632257\n",
      "source(train): つまり 同じ 地方 の コミュニティ 内 で 通用 する 言語 を 使用 し て いる <EOS>\n",
      "result(train): つまり 同じ 地方 の コミュニティ 内 で 通用 する 言語 を 使用 し て いる <EOS> も <EOS> <EOS> を\n",
      "epoch: 75/100 \n",
      "training loss: 0.0009737018845044076\n",
      "source(train): 主 な 格 助詞 と その 典型 的 な 機能 は 次 の 通り で ある <EOS>\n",
      "result(train): 主 な 格 助詞 と その 典型 的 な 機能 は 次 の 通り で ある <EOS> は は を\n",
      "epoch: 80/100 \n",
      "training loss: 0.0008684424101375043\n",
      "source(train): この 状態 で 虫 が つく こと も ほとんど 無い と 言う <EOS>\n",
      "result(train): この 状態 で 虫 が つく こと も ほとんど 無い と 言う <EOS> その <EOS> <EOS> その その その その\n",
      "epoch: 85/100 \n",
      "training loss: 0.0007906942628324032\n",
      "source(train): フランス 最大 の 都市 で あり 同国 の 政治 経済 文化 など の 中心 で ある <EOS>\n",
      "result(train): フランス 最大 の 都市 で あり 同国 の 政治 経済 文化 など の 中心 で ある <EOS> は は を\n",
      "epoch: 90/100 \n",
      "training loss: 0.0007300539873540401\n",
      "source(train): 事物 の 分布 を 考察 する にあたって 分布 図 の 作成 が 挙げ られる <EOS>\n",
      "result(train): 事物 の 分布 を 考察 する にあたって 分布 図 の 作成 が 挙げ られる <EOS> も <EOS> <EOS> を を\n",
      "epoch: 95/100 \n",
      "training loss: 0.0006807574536651373\n",
      "source(train): 漢語 は 音読み で 読ま れる こと から 字音 語 と 呼ば れる 場合 も ある <EOS>\n",
      "result(train): 漢語 は 音読み で 読ま れる こと から 字音 語 と 呼ば れる 場合 も ある <EOS> も ある が\n",
      "epoch: 100/100 \n",
      "training loss: 0.0006404282175935805\n",
      "source(train): 特に 移民 や 植民 地 など で フランス 色 が 強い 都市 に 多い <EOS>\n",
      "result(train): 特に 移民 や 植民 地 など で フランス 色 が 強い 都市 に 多い <EOS> は <EOS> <EOS> <EOS> を\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "学習を始める\n",
    "'''\n",
    "print(\"start train...\")\n",
    "auto_encoder=AutoEncoder(maxlen=MAX_LEN)\n",
    "auto_encoder.to(device)\n",
    "optimizer = optim.Adam(auto_encoder.parameters(), lr=ae_lr)\n",
    "history_train=[]\n",
    "history_eval=[]\n",
    "\n",
    "for epoch in range(AE_EPOCHS):\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttext確認用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nihongo_id=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「方言」の似ている単語\n",
      "方法:tensor(21.5570, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "ひとつ:tensor(21.7216, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "うち:tensor(21.8818, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "独自:tensor(21.9748, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "県庁:tensor(22.0037, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "品詞:tensor(22.0942, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "ゆれ:tensor(22.1198, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "日干し:tensor(22.1313, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "にくい:tensor(22.2012, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "生活:tensor(22.2619, device='cuda:0', grad_fn=<NormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "nihongo_id+=1\n",
    "norm_list=[]\n",
    "word=TEXT.vocab.itos[nihongo_id]\n",
    "nihongo_vector=share_conv.weight[nihongo_id]\n",
    "for i, val in enumerate(share_conv.weight):\n",
    "    diff=nihongo_vector-val\n",
    "    norm=torch.norm(diff)\n",
    "    norm_list.append((norm,i))\n",
    "norm_list.sort()\n",
    "print(\"「\"+word+\"」の似ている単語\")\n",
    "for i in range(1,11):\n",
    "    val=norm_list[i][0]\n",
    "    id_=norm_list[i][1]\n",
    "    print(TEXT.vocab.itos[id_]+\":\"+str(val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36-yasuda",
   "language": "python",
   "name": "py36-yasuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
